/*
 *
 *  Multi Process Garbage Collector
 *  Copyright Â© 2016 Hewlett Packard Enterprise Development Company LP.
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Lesser General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the 
 *  Application containing code generated by the Library and added to the 
 *  Application during this compilation process under terms of your choice, 
 *  provided you also meet the terms and conditions of the Application license.
 *
 */

#include <condition_variable>
#include <unordered_map>

#include "mpgc/gc_handshake.h"
#include "mpgc/gc_thread.h"
#include "mpgc/gc.h"

namespace mpgc {

  volatile bool request_gc_termination = false;
  static std::mutex gc_termination_mutex;
  static std::condition_variable gc_terminated;

  //Used by fault-tolerance code to determine which barrier will be the next one.
  static const Barrier_indices next_barrier_index_mapping[Barrier_indices::arraysize + 1] = {Barrier_indices::preMarking,
                                                                                             Barrier_indices::marking1,
                                                                                             Barrier_indices::sweep1,
                                                                                             Barrier_indices::sweep2,
                                                                                             Barrier_indices::postSweep1,
                                                                                             Barrier_indices::postSweep2,
                                                                                             Barrier_indices::sync,
                                                                                             Barrier_indices::preSweep};

  /*
   * Structure to be used by the cleanup functions which loop over processes
   * to determine if there is a dead process, and if so, then take appropriate
   * action.
   */
  template <typename T>
  struct barrier_id_dead_processes_t {
    std::deque<T> deque;
    bool found;
    barrier_id_dead_processes_t() : deque(), found(false) {}
  };
  template <typename T> using barrier_id_dead_processes_map = typename std::unordered_map<pcount_t, barrier_id_dead_processes_t<T>>;

  /* 
   * This function is to be called while looping to allocate a block from global allocator.
   * This is needed as otherwise a thread, which has deferred sweep signal, will never allow
   * sweep to progress, and will loop forever in the global allocator.
   */
  void global_allocation_epilogue() {
    gc_handshake::in_memory_thread_struct &thread_struct = *gc_handshake::thread_struct_handles.handle;

    /* We should continue to defer sweep signal until allocation_epilogue() is invoked.
     * This is to avoid any race that may arise otherwise.
     */
    thread_struct.sweep_signal_disabled = false;
    if (thread_struct.sweep_signal_requested) {
      thread_struct.sweep_signal_requested = false;
      gc_handshake::do_deferred_sweep_signal(thread_struct);
    }
    thread_struct.sweep_signal_disabled = true;

    if (thread_struct.clear_local_allocator) {
      thread_struct.clear_local_allocator = false;
      thread_struct.local_free_list.clear();
    }
  }

  /*
   * This function is called before allocation to defer sweep signal.
   */
  void allocation_prologue() {
    //Initialize the GC in the allocation path.
    initialize_thread();

    gc_handshake::in_memory_thread_struct &thread_struct = *gc_handshake::thread_struct_handles.handle;
    thread_struct.sweep_signal_disabled = true;

    if (thread_struct.clear_local_allocator) {
      thread_struct.clear_local_allocator = false;
      thread_struct.local_free_list.clear();
    }
  }

  /*
   * This function is called after the allocation and before construction.
   */
  void allocation_epilogue(void *p, gc_token &tok, std::size_t array_element_count) {
    gc_handshake::in_memory_thread_struct &thread_struct = *gc_handshake::thread_struct_handles.handle;
    assert(thread_struct.status_idx.load().status() != gc_handshake::Signum::sigInit);
    gc_control_block &cb = control_block();

    /* The following is required to avoid a race condition where the construction of gc_array_base
     * gets pre-empted by async_signal after the construction of gc_allocated, but before initializing
     * the array size.
     */
    *(static_cast<std::size_t*>(p) + 1) = array_element_count;

    /* We need the following signal_fence because the array_size *must* be
     * stored before constructing gc_descriptor. Otherwise, if it gets
     * re-ordered, and we get a async signal; the object size calculation
     * will be wrong.
     */
    std::atomic_signal_fence(std::memory_order_release);

    const offset_ptr<const gc_allocated> ptr = new (p) gc_allocated(tok);

    /* We need the following signal_fence because if we read the status
     * before the store above, get a async signal, and then make the above
     * store, then we will neither mark it, nor will be considered in the
     * stack scanning of the async signal handler.
     */
    std::atomic_signal_fence(std::memory_order_release);

    /*
     * Normally we mark end-bitmap first, followed by begin-bitmap. This way
     * if the process crashes before marking both the bits, we would not have
     * marked the object by then, as marked test is done using begin-bitmap.
     *
     * However, for application threads which are allocating black, we should
     * mark begin-bitmap first, as otherwise, the sweep algorithm doesn't work
     * correctly. However, setting only the end-bitmap (and not the begin-bitmap)
     * is tolerable by the sweep algorithm. Also, marked test is never done on
     * these new allocations which are not even being pointed by any field in the
     * heap.
     */
    if (thread_struct.status_idx.load().status() == gc_handshake::Signum::sigAsync) {
      cb.bitmap.mark_begin_first(ptr);
    }
    cb.mem_stats.marked(ptr);

    /* We need the following signal_fence because sweep signal *must* not be
     * enabled (or processed) before marking, if we are in async phase.
     */
    std::atomic_signal_fence(std::memory_order_release);

    //Enable sweep signal, and process if already pending.
    thread_struct.sweep_signal_disabled = false;
    if (thread_struct.sweep_signal_requested) {
      thread_struct.sweep_signal_requested = false;
      gc_handshake::do_deferred_sweep_signal(thread_struct);
    }
  }

  /*
   * Function to capture root pointers, both, external_gc_ptrs and persistent roots.
   */
  static void capture_global_roots(gc_control_block &cb, Traversal_queue &q) {
    inbound_pointers::inbound_table::table(true)->for_each_slot([&q, &cb](const offset_ptr<const gc_allocated> p) {
      if (p.is_valid() && !cb.bitmap.is_marked(p)) {
        //q.push_front(p);
          q.push(p);
      }
    });

    cb.persistent_roots
      .enumerate_pointers([&q, &cb](const gc_ptr<const gc_allocated> &r) {
          if (r != nullptr) {
            offset_ptr<const gc_allocated> p = r.as_offset_pointer();
            if (p.is_valid()  && !cb.bitmap.is_marked(p)) {
              //q.push_front(p);
              q.push(p);
            }
          }
        });
  }
  /*
   * The main function that marks black an object. It enumerates all the pointers in the object and
   * then marks it.
   */
  static void mark_black(const offset_ptr<const gc_allocated> &p, gc_control_block &cb, Traversal_queue &q) {
    //assert(p.is_valid() && p->get_gc_descriptor().is_valid());
    //assert(p.is_valid());
    if (!p.is_valid() || cb.bitmap.is_marked(p) || !p->get_gc_descriptor().is_valid()) {
      return;
    }
    p->get_gc_descriptor().for_each_ref([&q, &cb](const base_offset_ptr *base_ptr) {
      const offset_ptr<const gc_allocated> ptr = *base_ptr;
      if (!ptr.is_null()) {
        assert(ptr.is_valid());
        bool marked = cb.bitmap.is_marked(ptr);
        if (ptr.is_weak()) {
          if (!marked) {
            cb.bitmap.mark_weak((std::size_t*)base_ptr);
          }
        } else if (!marked) {
          assert(ptr->get_gc_descriptor().is_valid());
          q.push(ptr);
        }
      }
    });
    /*
     * We mark end-bitmap first so that in case we crash before marking begin-bitmap,
     * the object is not considered marked, and whichever process takes over the
     * unfinished work of this process, will mark it.
     */
    cb.bitmap.mark_end_first(p);
    // We should probably do this in the per-process struct and sweep
    // the total into the global when we try to increment the cycle
    // number, but that's more bookkeeping and it might be more work
    // to get the reference to the per-process struct.
    cb.mem_stats.marked(p);
  }

  /*
   * The function to increment the given barrier. For fault-tolerance,
   * the value before incrementing is stored in persistent space as an
   * ID. We cannot use the regular atomic increment, because there is
   * a possibility that increment takes place but ID is not stored in
   * the persistent space. So we use CAS.
   */
  static void inc_barrier(per_process_struct &p, const Barrier_indices n) {
    gc_control_block &cb = control_block();
    pcount_t &i = p.barrier_id_ref();

    p.set_barrier_incrementing();
    assert(i == 0);
    i = cb.barrier_sync[n];
    while (!cb.barrier_sync[n].compare_exchange_weak(i, i + 1));
    //assert(i <= cb.total_process_count.load().count);
    p.set_barrier_incremented();
  }

  /*
   * Function called by GC thread in the marking phase to empty the work queue.
   * We do *push* based work-load balancing, wherein, overloaded thread *offers*
   * work to idle ones.
   */
  static bool empty_collector_stack(gc_control_block &cb, per_process_struct &p, Traversal_queue &q) {
    bool worked = false;
    while (!q.empty()) {
      worked = true;

      p.set_marking_ref(q.front());
      q.pop();
      mark_black(p.marking_ref(), cb, q);
      p.clear_marking_ref();
      if (request_gc_termination) {
        break;
      }
    }
    return worked;
  }

  static void consume_dead_process_refs(per_process_struct &process_struct, Traversal_queue &my_q) {
    Traversal_queue &q = process_struct.traversal_queue();
    Mark_buffer_list &mb_list = process_struct.mark_buffer_list();
    Mbuf *m = mb_list.head();
    while (m) {
      while (!m->is_empty()) {
        m->process_element([&my_q] (const offset_ptr<const gc_allocated> &p) {
          my_q.push(p);
        });
      }
      m = mb_list.next(m);
    }
    my_q.push(process_struct.marking_ref());
    my_q.takeover_locals(q);
  }

  template <typename Func, typename ...Args>
  static pcount_t cleanup_failures(void (*dead_action)(per_process_struct*), Func &&cleanup_func, Args&& ...args) {
    gc_control_block &cb = control_block();
    pcount_t nr_dead_process = 0;
    pcount_t nr_total_process = 0;
    per_process_struct *p = gc_handshake::process_struct;
    barrier_id_dead_processes_map<per_process_struct*> map;

    assert(!map[p->get_barrier_id()].found);
    map[p->get_barrier_id()].found = true;
    do {
      nr_total_process++;
      p = cb.process_struct_list.next(p);
      if (p == nullptr) {
        p = cb.process_struct_list.head();
      }
      if (p == gc_handshake::process_struct) {
        break;
      } else if (request_gc_termination) {
        return 0;
      }

      per_process_struct::liveness old_liveness = p->get_liveness();
      per_process_struct::Barrier_info binfo = p->get_barrier_info();

      if (old_liveness.is_live == per_process_struct::Alive::Dead) {
        nr_dead_process++;
        continue;
      } else if (binfo._info.index == next_barrier_index_mapping[gc_handshake::process_struct->get_barrier_index()]) {
        return 0;
      } else if (per_process_struct::get_creation_time(old_liveness.pid) != old_liveness.creation_time) {
        if (binfo._info.index != gc_handshake::process_struct->get_barrier_index()) {
          //The following commented code is required only if there is a possibility of the same barrier is used back-to-back.
          //proc.reset_barrier_info(proc.get_barrier_index());
          dead_action(p);
          nr_dead_process++;
          continue;
        } else if (binfo._info.bstage == Barrier_stage::unincremented) {
          if (std::forward<Func>(cleanup_func)(p, old_liveness, std::forward<Args>(args)...)) {
            //The function is suppose to return true if dead process count must be incremented.
            nr_dead_process++;
          }
          continue;
        } else if (binfo._info.bstage == Barrier_stage::incrementing) {
          barrier_id_dead_processes_t<per_process_struct*> &barrier_id_value = map[binfo._info.barrier];
          if (barrier_id_value.found) {
            nr_dead_process++;
            dead_action(p);
          } else {
            barrier_id_value.deque.push_front(p);
          }
          continue;
        }
      }
      //if it has incremented already, it should be accounted for.
      if (binfo._info.bstage == Barrier_stage::incremented &&
          binfo._info.index == gc_handshake::process_struct->get_barrier_index()) {
        barrier_id_dead_processes_t<per_process_struct*> &barrier_id_value = map[binfo._info.barrier];
        assert(!barrier_id_value.found);
        barrier_id_value.found = true;
        nr_dead_process += barrier_id_value.deque.size();
        while(!barrier_id_value.deque.empty()) {
          per_process_struct *proc = barrier_id_value.deque.front();
          dead_action(proc);
          barrier_id_value.deque.pop_front();
        }
      }
    } while (true);

    pcount_t curr_barrier_count = cb.barrier_sync[gc_handshake::process_struct->get_barrier_index()];
    while (!map.empty()) {
      barrier_id_dead_processes_t<per_process_struct*> &barrier_id_value = map.begin()->second;
      if (!barrier_id_value.found) {
        assert(barrier_id_value.deque.size() > 0);
        nr_dead_process += barrier_id_value.deque.size() - 1;
        if (map.begin()->first == curr_barrier_count) {
          nr_dead_process++;
          while(!barrier_id_value.deque.empty()) {
            per_process_struct *proc = barrier_id_value.deque.front();
            dead_action(proc);
            barrier_id_value.deque.pop_front();
          }
        }
      }
      map.erase(map.begin());
    }
    return nr_total_process - nr_dead_process;
  }

  static pcount_t cleanup_marking_failures(gc_control_block &cb, Traversal_queue &my_q) {
    pcount_t nr_dead_process = 0;
    pcount_t nr_total_process = 0;
    per_process_struct *p = gc_handshake::process_struct;
    using map_pair = std::pair<per_process_struct*, per_process_struct::liveness>;
    barrier_id_dead_processes_map<map_pair> map;
    assert(!map[p->get_barrier_id()].found);
    map[p->get_barrier_id()].found = true;
    do {
      nr_total_process++;
      p = cb.process_struct_list.next(p);
      if (p == nullptr) {
        p = cb.process_struct_list.head();
      }
      if (p == gc_handshake::process_struct) {
        break;
      } else if (request_gc_termination) {
        return 0;
      }

      per_process_struct::liveness old_liveness = p->get_liveness();
      per_process_struct::Barrier_info binfo = p->get_barrier_info();
      if (old_liveness.is_live == per_process_struct::Alive::Dead) {
        nr_dead_process++;
        continue;
      } else if (binfo._info.version > gc_handshake::process_struct->get_barrier_version() ||
                 (binfo._info.version == gc_handshake::process_struct->get_barrier_version() &&
                  binfo._info.index == Barrier_indices::marking2)) {
        return 0;
      } else if (per_process_struct::get_creation_time(old_liveness.pid) != old_liveness.creation_time) {
        per_process_struct &proc = *p;
        per_process_struct::liveness desired = gc_handshake::process_struct->get_liveness();
        if (proc.set_liveness(old_liveness, desired)) {
          consume_dead_process_refs(proc, my_q);
          bool ret = proc.set_liveness(desired, old_liveness);
          assert(ret);
        }
        if (binfo._info.version < gc_handshake::process_struct->get_barrier_version() ||
            (binfo._info.index != Barrier_indices::marking1 && binfo._info.index != Barrier_indices::marking2)) {
          //The following commented code is required only if there is
          //a possibility of the same barrier is used back-to-back.

          //proc.reset_barrier_info(proc.get_barrier_index());

          //In this function, if we fail to mark_dead means somebody
          //either is still working on cleaning up a dead process, or
          //has died while working on it. Therefore, at anytime this
          //function returns false, we must return out and try fresh.
          if (!proc.mark_dead(old_liveness)) {
            return 0;
          }
          nr_dead_process++;
          continue;
        } else if (binfo._info.index == Barrier_indices::marking2) {
          assert(false);
        } else {
          assert(binfo._info.index == Barrier_indices::marking1);
          assert(binfo._info.version == gc_handshake::process_struct->get_barrier_version());
          if (binfo._info.bstage == Barrier_stage::unincremented) {
            if (!proc.mark_dead(old_liveness)) {
              return 0;
            }
            nr_dead_process++;
            continue;
          } else if (binfo._info.bstage == Barrier_stage::incrementing) {
            barrier_id_dead_processes_t<map_pair> &barrier_id_value = map[binfo._info.barrier];
            if (barrier_id_value.found) {
              if (proc.mark_dead(old_liveness)) {
                return 0;
              }
              nr_dead_process++;
            } else {
              barrier_id_value.deque.push_front(map_pair(p, old_liveness));
            }
            continue;
          }
        }
      }

      //if it has incremented already, it should be accounted for.
      if (binfo._info.bstage == Barrier_stage::incremented &&
          binfo._info.version == gc_handshake::process_struct->get_barrier_version() &&
          binfo._info.index == Barrier_indices::marking1) {
        //For processes with version < our version, consider it as unincremented and hence don't do
        //anything, just continue to next process.
        barrier_id_dead_processes_t<map_pair> &barrier_id_value = map[binfo._info.barrier];
        assert(!barrier_id_value.found);
        barrier_id_value.found = true;
        nr_dead_process += barrier_id_value.deque.size();
        while(!barrier_id_value.deque.empty()) {
          map_pair &pair = barrier_id_value.deque.front();
          if (!pair.first->mark_dead(pair.second)) {
            return 0;
          }
          barrier_id_value.deque.pop_front();
        }
      }
    } while (true);

    marking_barrier_type curr_barrier_count = cb.marking_barrier;
    while (!map.empty()) {
      barrier_id_dead_processes_t<map_pair> &barrier_id_value = map.begin()->second;
      if (!barrier_id_value.found) {
        assert(barrier_id_value.deque.size() > 0);
        nr_dead_process += barrier_id_value.deque.size() - 1;
        if (map.begin()->first == curr_barrier_count._info.barrier) {
          nr_dead_process++;
          while(!barrier_id_value.deque.empty()) {
            map_pair &pair = barrier_id_value.deque.front();
            if (!pair.first->mark_dead(pair.second)) {
              return 0;
            }
            barrier_id_value.deque.pop_front();
          }
        }
      }
      map.erase(map.begin());
    }
    return nr_total_process - nr_dead_process;
  }

  static bool help_other_processes(gc_control_block &cb, per_process_struct &process_struct, Traversal_queue &q) {
    per_process_struct *p = &process_struct;
    bool helped = false;
    do {
      p = cb.process_struct_list.next(p);
      if (p == nullptr) {
        p = cb.process_struct_list.head();
      }
      if (p == gc_handshake::process_struct) {
        break;
      } else if (request_gc_termination) {
        //Return true to terminate at the earliest possible.
        return helped;
      }

      while (p->steal(q)) {
        helped = true;
        empty_collector_stack(cb, process_struct, q);
        if (request_gc_termination) {
          return helped;
        }
      }
    } while (true);
    return helped;
  }

  static void marking_phase(per_process_struct &process_struct) {
    gc_control_block &cb = control_block();
    Traversal_queue &q = process_struct.traversal_queue();
    gc_handshake::in_memory_thread_struct_list_type &thread_list = gc_handshake::thread_struct_list;

    marking_barrier_type local_barrier;
    versioned_pcount_t nr_live_process;
    std::size_t iter = 0;
    uint32_t curr_version = 0;
    uint16_t weak_stage_ver = 0;
    bool clean = false;
    bool do_handshake = false;
    uint8_t spin_count;

    constexpr auto version_bits_fld = bits::field<uint16_t, uint16_t>(2, 14);
    constexpr auto stage_bits_fld = bits::field<Weak_stage, uint16_t>(0, 2);
    while (true) {
      while (!clean) {
        if (iter++ > 0) {
          if (process_struct.gc_mutator_weak_sync == 1) {
            //If gc_mutator_weak_sync is <0, stop> then change it to <0, work>.
            process_struct.gc_mutator_weak_sync = 0;
          }
          uint16_t expected = stage_bits_fld.encode(Weak_stage::Repeat) |
                              version_bits_fld.encode(weak_stage_ver);
          uint16_t desired = stage_bits_fld.replace(expected, Weak_stage::Trace);

          while (stage_bits_fld.decode(expected) == Weak_stage::Repeat &&
                 !cb.weak_stage.compare_exchange_strong(expected, desired)) {
            weak_stage_ver = version_bits_fld.decode(expected);
            desired = stage_bits_fld.replace(expected, Weak_stage::Trace);
          }
          do_handshake = true;
        }
	process_struct.reset_barrier_info(Barrier_indices::marking1);
	while (true) {
	  if (request_gc_termination) {
	    return;
	  }
	  while (!clean) {
	    clean = true;
            gc_handshake::in_memory_thread_struct *t = thread_list.head();
	    while (t) {
              if (do_handshake) {
                gc_handshake::Weak_signal expected_weak_signal = gc_handshake::Weak_signal::InBarrier;
                t->weak_signal.compare_exchange_strong(expected_weak_signal, gc_handshake::Weak_signal::DoHandshake);
              }
	      Mbuf *m = t->mbuffer;
	      while (!m->is_empty()) {
		clean = false;
		m->process_element(mark_black, cb, q);
		if (request_gc_termination) {
		  return;
		}
	      }
	      t = thread_list.next(t);
	    }
            do_handshake = false;
	    empty_collector_stack(cb, process_struct, q);
	  }
	  // Let's help others.
	  if (!help_other_processes(cb, process_struct, q)) {
	    // Nobody needed help.
	    break;
	  }
	  /* Spent some time helping others. Let's check again if
	   * some mutator(s) have unfinished business.
	   */
	} //while(true)

	spin_count = 0;
	assert(clean);
	if (request_gc_termination) {
	  return;
	}

	{
	  marking_barrier_type &desired = local_barrier;
	  marking_barrier_type &expected = process_struct.marking_barrier_ref();
	  expected = cb.marking_barrier;
	  assert(expected._info.bstage == Barrier_stage::incrementing);
	  curr_version = expected._info.version;
	  do {
	    //assert(desired.barrier <= cb.total_process_count.load().count);
	    assert(curr_version == expected._info.version);
	    desired = expected;
	    //This might happen if all existing processes moved to marking2 and then
	    //a new process comes in and tries to increment marking1.
	    if (expected._info.index == Barrier_indices::marking2) {
              assert(iter == 1 && process_struct.gc_mutator_weak_sync == 1);
              //We use the flag for something other than what the name reflects
              //as it's needed only once.
              do_handshake = true;
	      break;
	    }
	    desired._info.barrier++;
	  } while (!cb.marking_barrier.compare_exchange_weak(expected, desired));
          if (!do_handshake) {
            process_struct.set_barrier_incremented();
          }
	}

	/* We don't need to check for stage == Stage::Tracing here
	 * because there is no way that any GC thread goes past Marking2
	 * barrier.
	 */
	nr_live_process = cb.total_process_count;
	while (local_barrier._info.barrier < nr_live_process.count &&
	       local_barrier._info.index == Barrier_indices::marking1 &&
	       local_barrier._info.version == curr_version) {
	  assert(cb.stage == Stage::Tracing);
	  if (++spin_count == 0) {
	    // First help others.
	    if (!help_other_processes(cb, process_struct, q)) {
	      // If nobody needed help, then check for failures.
	      versioned_pcount_t temp_live_process(cleanup_marking_failures(cb, q), nr_live_process.version + 1);
	      //temp_live_process = 0 indicates that termination has been requested.
	      if (temp_live_process.count > 0 && cb.total_process_count.compare_exchange_strong(nr_live_process, temp_live_process)) {
		nr_live_process = temp_live_process;
	      }
	      /* Set clean to false if work done. Otherwise, leave it as is.
	       * It's important to have empty_collector_stack as the first
	       * operand to && so that the function is called no matter what.
	       */
	      clean = !empty_collector_stack(cb, process_struct, q) && clean;
	    } else {
	      /* This needs some explanation! Consider a scenario where GC thread 1
	       * after incrementing marking barrier 1 steals work from GC thread 2.
	       * This may cause GC thread 2 to think that its work is done and hence
	       * will increment marking barrier 1, which may meet the marking barrier 1's
	       * criteria. Thereafter, GC thread 2 will check its mark-buffers, find them
	       * empty and then increment marking barrier 2. At this time, GC thread 1
	       * may be working on the stolen work. But during the same time, if some
	       * mutator(s) in the GC thread 2's process update(s) a reference among
	       * white references, it will cause references to be added to the mark-
	       * buffer(s), which may go unnoticed because its GC thread is already
	       * at marking barrier 2 and GC thread 1 will eventually finish the stolen
	       * work, may find its mark-buffers empty, and hence increment the marking
	       * barrier 2.
	       *
	       * To solve this, we make GC thread 1 pretend as if it found in its mark
	       * buffers after exiting marking barrier 1. This will make all GC threads
	       * to go back for another round of marking, when GC thread 2 will find
	       * unprocessed references in marking buffers, if any.
	       */
	      clean = false;
	    }
	  }

	  if (request_gc_termination) {
	    return;
	  }
	  std::cpu_relax();
	  nr_live_process = cb.total_process_count;
	  local_barrier = cb.marking_barrier;
	}

	assert(q.empty());
	process_struct.reset_barrier_info(Barrier_indices::marking2);
	if (local_barrier._info.version != curr_version) {
	  /* This indicates that somebody has already incremented the version.
	   * We don't need to do anything now. Just continue.
	   */
	  clean = false;
	  continue;
	} else if (clean) {
          uint16_t expected = 0;
          if (process_struct.gc_mutator_weak_sync.compare_exchange_strong(expected, 1)) {
            gc_handshake::in_memory_thread_struct *t = thread_list.head();
	    while (t) {
              while (t->weak_signal == gc_handshake::Weak_signal::DoHandshake) {
                std::cpu_relax();
              }
              Mbuf *m = t->mbuffer;
	      if (!m->is_empty()) {
	        clean = false;
                //Not sure if we can exit immediately since there might still be some mutator(s)
                //which have not yet handshaked.
	        break;
	      }
              t = thread_list.next(t);
	    }
          } else if (!do_handshake) {
            //Some mutator is in the read barrier right now. Go back
            clean = false;
          }
	}

	if (request_gc_termination) {
	  return;
	}

	if (!clean) {
	  marking_barrier_type desired(Barrier_stage::incrementing, Barrier_indices::marking1);
	  desired._info.version = local_barrier._info.version + 1;
	  assert(desired._info.barrier == 0);
	  while (local_barrier._info.version != desired._info.version &&
		 !cb.marking_barrier.compare_exchange_strong(local_barrier, desired));
	} else {
	  if (!do_handshake) {
	    //First set the marking barrier to <marking2, ver, 0> from <marking1, ver, n>
	    marking_barrier_type desired(Barrier_stage::incrementing, Barrier_indices::marking2);
	    desired._info.version = local_barrier._info.version;
	    assert(desired._info.barrier == 0);
	    while (local_barrier._info.index != Barrier_indices::marking2 &&
		   !cb.marking_barrier.compare_exchange_strong(local_barrier, desired)) {
	      if (local_barrier._info.version != curr_version) {
		clean = false;
		break;
	      }
	    }
	  }
	  if (clean) {
	    //Now we try to increment.
	    marking_barrier_type &desired = local_barrier;
	    marking_barrier_type &expected = process_struct.marking_barrier_ref();
	    expected = cb.marking_barrier;
	    assert(expected._info.bstage == Barrier_stage::incrementing);

	    do {
	      if (expected._info.version != curr_version) {
		assert(expected._info.index == Barrier_indices::marking1);
		process_struct.set_barrier_unincremented();
		clean = false;
		//Someone wants to go back to marking. Let's join him.
		break;
	      }
	      assert(expected._info.index == Barrier_indices::marking2);
	      desired = expected;
	      desired._info.barrier++;
	    } while (!cb.marking_barrier.compare_exchange_weak(expected, desired));
	  }

	  if (clean) {
	    process_struct.set_barrier_incremented();
	    spin_count = 0;
	    while (local_barrier._info.barrier < cb.total_process_count.load().count &&
                   //We need to check for this as the count may not match due to some new process, while
                   //the last existing process might have already incremented and gone past Tracing stage.
		   cb.stage == Stage::Tracing) {
	      std::cpu_relax();
	      local_barrier = cb.marking_barrier;
              if (local_barrier._info.index == Barrier_indices::marking1) {
                //Some other process has asked to go back to marking.
                //There is nothing to fix, just return back to top.
                clean = false;
                break;
	      } else if (++spin_count == 0) {
		pcount_t temp_live_process = cleanup_failures([](per_process_struct *p) {return;},
							      [](per_process_struct *p,
								 per_process_struct::liveness &expected) {return true;});
		if (temp_live_process > 0 && cb.total_process_count.load().count != temp_live_process) {
		  //Someone died. We need to cleanup. Lets go do marking again.
		  clean = false;
		  marking_barrier_type desired(Barrier_stage::incrementing, Barrier_indices::marking1);
		  desired._info.version = local_barrier._info.version + 1;
		  assert(desired._info.barrier == 0);
		  while (local_barrier._info.version != desired._info.version &&
			 !cb.marking_barrier.compare_exchange_strong(local_barrier, desired));
		}
	      }
	      if (request_gc_termination) {
		return;
	      }
	    }
	  }
	}
      } //while(!clean)
      assert(clean);
      //do the stage cas here to claim that tracing is done.
      uint16_t expected_weak_stage = stage_bits_fld.encode(Weak_stage::Trace) |
                                     version_bits_fld.encode(weak_stage_ver);
      weak_stage_ver++;
      if (cb.weak_stage.compare_exchange_strong(expected_weak_stage,
                                                stage_bits_fld.encode(Weak_stage::Clean) |
                                                version_bits_fld.encode(weak_stage_ver)) ||
          stage_bits_fld.decode(expected_weak_stage) == Weak_stage::Clean) {
        break;
      } else {
        //Some mutator beat us to changing stage to Repeat. Reset the marking barrier.    
        marking_barrier_type desired(Barrier_stage::incrementing, Barrier_indices::marking1);
        desired._info.version = local_barrier._info.version + 1;
        assert(desired._info.barrier == 0);
        while (local_barrier._info.version != desired._info.version &&
               !cb.marking_barrier.compare_exchange_strong(local_barrier, desired));
        //We need to reset clean to process another cycle.
        clean = false;
      }
    } //while(true)
    process_struct.reset_barrier_info(Barrier_indices::preSweep);

    assert(q.empty());
    Mark_buffer_list &mb_list = process_struct.mark_buffer_list();
    for (Mbuf *m = mb_list.head(); m; m = mb_list.next(m)) {
      while (!m->is_empty()) {
        m->process_element([&cb] (const offset_ptr<const gc_allocated> &p) {
          assert(cb.bitmap.is_marked(p));
        });
      }
    }
  }

  static void post_sweep_weak_ptr_sync(gc_control_block &cb, per_process_struct *proc) {
    constexpr auto stage_bits_fld = bits::field<Weak_stage, uint16_t>(0, 2);
    gc_handshake::in_memory_thread_struct_list_type &thread_list = gc_handshake::thread_struct_list;
    bool should_check = false;

    cb.weak_stage = stage_bits_fld.encode(Weak_stage::Normal);

    gc_handshake::in_memory_thread_struct *t = thread_list.head();
    while (t) {
      gc_handshake::Weak_signal expected_weak_signal = gc_handshake::Weak_signal::InBarrier;
      if (t->weak_signal.compare_exchange_strong(expected_weak_signal, gc_handshake::Weak_signal::DoHandshake)) {
        should_check = true;
      }
      if (request_gc_termination) {
        return;
      }
      t = thread_list.next(t);    
    }

    if (should_check) {
      t = thread_list.head();
      while (t) {
        while (t->weak_signal == gc_handshake::Weak_signal::DoHandshake) {
          if (request_gc_termination) {
            return;
          }
          std::cpu_relax();
        }
        t = thread_list.next(t);
      }
    }
  }

  static void erase_gc_descriptors_from_free_chunk(std::size_t* begin,
                                                   const std::size_t size_in_words) {
    static_assert(sizeof(std::size_t) == sizeof(gc_allocated), "sizeof gc_allocated larger than size_t");
    const std::size_t * const end = begin + size_in_words;

    for (std::size_t size = 0; begin < end; begin += size) {
      const gc_descriptor &gc_desc = reinterpret_cast<gc_allocated*>(begin)->get_gc_descriptor();
      if (!gc_desc.is_illegal()) {
        size = gc_desc.object_size();
        *begin = size << 3;
      } else {
        size = *begin >> 3;
      }
      assert(size != 0);
    }
    assert(begin == end);
  }

  static void put_to_global(gc_allocator::globalListType& list, const std::size_t beg_word, const std::size_t size_in_bytes) {
    /*
     * There are 2 ways that we can get a free chunk smaller than global_chunk (or local_chunk).
     * 1) If the chunk happens to be the first word of an expanded chunk for which we intentionally
     * left the first word, and instead marked the second word. In this case, we can return as this
     * free chunk is already added to the allocator as part of the expanded chunk.
     *
     * 2) If the chunk was left unused by allocator due to allocation request being just one word
     * smaller than the allocation chunk, then also we don't need to do anything as the allocator
     * ensures that the size of the chunk is already written there, which is very important for
     * correct working of erase_gc_descriptors_from_free_chunk().
     *
     * So in any of the case, we can simply return without doing anything.
     */
    if (size_in_bytes < sizeof(gc_allocator::global_chunk)) {
      return;
    }
    std::size_t *begin = reinterpret_cast<std::size_t*>(base_offset_ptr::base()) + beg_word;
    erase_gc_descriptors_from_free_chunk(begin, size_in_bytes >> 3);

    gc_allocator::put_to_global(list, size_in_bytes,
                                new (begin) gc_allocator::global_chunk(size_in_bytes));
  }

  bool mark_bitmap::expand_free_chunk(std::size_t *heap_begin,
                                      offset_ptr<gc_allocator::global_chunk> c,
                                      std::size_t size,
                                      std::size_t &beg_word,
                                      std::size_t &end_word) {
    beg_word = c.offset() >> 3;
    end_word = beg_word + (size >> 3);

    beg_word = find_prev_used_word(beg_word);
    end_word = find_next_used_word(end_word, _size << value_log_bits);
    //find_next_used_word returns the next used word. So we must decrement by 1.

    /* Q: Why add 1 to beg_word?
     * A: Because in case beg_word corresponds to a just collected object's
     *    beginning and we are suppose to clear some weak_gc_ptr to this
     *    object, then we will not be able to do so as the weak ptr clearing
     *    logic works entirely on the check whether the corresponding object
     *    is marked in the bitmaps or not.
     *    In order to get around this, we mark the second word of the expanded
     *    chunk, rather than the first one. Since all threads will try to do so,
     *    it resolves the concurrency.
     */
    const bool is_not_obj = reinterpret_cast<gc_allocated*>(heap_begin + beg_word)->get_gc_descriptor().is_illegal();
    return _mark_begin_first((beg_word + (is_not_obj ? 0 : 1)) << 3, (end_word - 1) << 3);
  }

  void sweep1_phase() {
    gc_control_block &cb = control_block();
    gc_allocator::globalListType &other_list = cb.global_free_list[1 - gc_handshake::process_struct->global_list_index()];
    Pre_sweep_list &pre_sweep_list = gc_handshake::process_struct->pre_sweep_list();
    std::size_t *heap_begin = reinterpret_cast<std::size_t*>(base_offset_ptr::base());
    bool work_done = false;
    assert(pre_sweep_list.empty());
    for (uint8_t i = gc_allocator::global_list_size(); !work_done && i > 5; i--) {
      do {
        if (request_gc_termination) {
          return;
        }
        offset_ptr<gc_allocator::global_chunk> c = gc_allocator::get_chunk_from_global(other_list, i - 1);
        if (c == nullptr) {
          break;
        } else {
          work_done = true;
        }
        std::size_t beg_word, end_word;
        if (cb.bitmap.expand_free_chunk(heap_begin, c, c->size(), beg_word, end_word)) {
          /* We have to be carefull here. We are pushing beg_word and end_word as
           * two different iteruts in the deque. But they are really meant for single
           * operation. Therefore, while taking them out, firstly, the size of deque
           * should be in multiples of two. And, secondly, two pop operations should
           * be done from the other front to retrieve the beg_word and end_word in
           * right order.
           */
          pre_sweep_list.push_back(beg_word);
          pre_sweep_list.push_back(end_word);
        }
      } while(true);
    }
  }

  static void adjust_inbound_weak_pointers(gc_control_block &cb) {
    inbound_pointers::inbound_weak_table::table(true)->for_each_slot([&cb](auto &wp) {
      cb.bitmap.atomic_cleanup_weak_ptr(reinterpret_cast<size_t*>(&wp));
    });
  }

  void mark_bitmap::atomic_cleanup_weak_ptr(std::size_t *p) {
    constexpr auto ptr_fld = bits::field<std::size_t, std::size_t>(0, base_offset_ptr::used_bits());
    std::atomic<std::size_t> *atomic = reinterpret_cast<std::atomic<std::size_t>*>(p);
    std::size_t bare_exp = atomic->load();
    base_offset_ptr exp = bare_exp;
    assert(exp.is_null() || (exp.is_weak() && exp.is_valid()));
    while (!exp.is_null() &&
           !is_marked(&exp) &&
           !atomic->compare_exchange_weak(bare_exp, ptr_fld.replace(bare_exp, 0))) {
      exp = bare_exp;
      assert(exp.is_null() || (exp.is_weak() && exp.is_valid()));
    }
  }

  void mark_bitmap::cleanup_weak_ptr(std::size_t *p) {
    constexpr auto ptr_fld = bits::field<std::size_t, std::size_t>(0, base_offset_ptr::used_bits());
    base_offset_ptr *off_ptr = reinterpret_cast<base_offset_ptr*>(p);
    assert(off_ptr->is_null() || (off_ptr->is_weak() && off_ptr->is_valid()));
    if (!off_ptr->is_null() && !is_marked(off_ptr)) {
      *p = ptr_fld.replace(*p, 0);
    }
  }

  void mark_bitmap::verify_weak_ptr_cleanup(std::size_t *p) {
    base_offset_ptr *ptr = reinterpret_cast<base_offset_ptr*>(p);
    assert(ptr->is_null() || (ptr->is_weak() && ptr->is_valid() && is_marked(ptr)));
  }

  void mark_bitmap::cleanup_weak_ptrs(std::size_t begin, const std::size_t end) {
    const bit_number_t begin_bit = compute_bit_number(begin << 3);
    bitmap_idx_t begin_idx = compute_bitmap_index(begin << 3);
    const bit_number_t end_bit = compute_bit_number(end << 3);
    const bitmap_idx_t end_idx = compute_bitmap_index(end << 3);
    assert(begin_idx < end_idx || (begin_idx == end_idx && begin_bit <= end_bit));
    std::size_t * const base = reinterpret_cast<std::size_t*>(base_offset_ptr::base());

    auto func = [this, base, end](rep_t val, std::size_t begin) {
      while (val) {
        std::size_t clz = __builtin_clzl(val);
        assert(clz < bits_per_value);
        //NOTE:Shifting 64-bit is undefined. So we don't B <<= clz + 1.
        val <<= clz;
        begin += clz;
        atomic_cleanup_weak_ptr(base + begin);
        assert(begin <= end);
        begin++;
        val <<= 1;
      }
    };

    rep_t B = _weak[begin_idx] & construct_left_mask(begin_bit);
    while (begin_idx < end_idx) {
      func(B, begin_idx << value_log_bits);
      B = _weak[++begin_idx];
    }
    //Now the last bitmap word.
    func(B & construct_right_mask(end_bit), begin_idx << value_log_bits);
  }

  void mark_bitmap::verify_weak_ptrs_cleanup() {
    bitmap_idx_t begin_idx = 0;
    std::size_t * const base = reinterpret_cast<std::size_t*>(base_offset_ptr::base());

    auto func = [this, base](rep_t val, std::size_t begin) {
      while (val) {
        std::size_t clz = __builtin_clzl(val);
        assert(clz < bits_per_value);
        //NOTE:Shifting 64-bit is undefined. So we don't B <<= clz + 1.
        val <<= clz;
        begin += clz;
        verify_weak_ptr_cleanup(base + begin);
        begin++;
        val <<= 1;
      }
    };

    rep_t B = _weak[begin_idx];
    while (begin_idx < _size) {
      func(B, begin_idx << value_log_bits);
      B = _weak[++begin_idx];
    }
  }

  void mark_bitmap::process_logical_chunk(gc_allocator::globalListType &list, const std::size_t nr_chunk, const bool set_bit) {
    std::size_t first = 0;
    const std::size_t end = (nr_chunk + 1) << (chunk_size_log_bits + value_log_bits);
    std::size_t second;

    if (nr_chunk == 0) {
      second = find_next_used_word(first, end);
      if (second == end) {
        set_sweep_bitmap_both(0, set_bit);
        second = process_next_chunk_begin(1, set_bit);
        put_to_global(list, first, (second - first) << 3);
        return;
      } else {
        put_to_global(list, first, (second - first) << 3);
      }
    } else {
      second = nr_chunk << (chunk_size_log_bits + value_log_bits);
    }

    bool dirty_end_bitmap = false;
    while (second < end) {
      first = find_next_free_word(second, end, dirty_end_bitmap);
      //Go through weak-bitmap to fix weak ptrs from second to first.
      cleanup_weak_ptrs(second, first - 1);
      if (first == end) {
        rep_t B = _end[((nr_chunk + 1) << chunk_size_log_bits) - 1];
        if (B & 0x1) {
          //If the last word of this chunk was end of an object, then we have to process the next chunk's _begin
          second = process_next_chunk_begin(nr_chunk + 1, set_bit);
        } else {
          break;//There is nothing to add in the free list.
        }
      } else {
        second = find_next_used_word(first, end);
        if (second == end) {
          second = process_next_chunk_begin(nr_chunk + 1, set_bit);
        }
      }
      put_to_global(list, first, (second - first) << 3);
    }

    if (!dirty_end_bitmap) {
      set_sweep_bitmap_end(nr_chunk, set_bit);
    }
  }

  void mark_bitmap::_set_sweep_bitmap_range(const std::size_t start_chunk, const std::size_t finish_chunk, const bool set) {
    std::size_t start_word_idx = start_chunk >> value_log_bits;
    const std::size_t finish_word_idx = finish_chunk >> value_log_bits;

    const rep_t first_chunk_desired = (rep_t(1) << (bits_per_value - (start_chunk & (bits_per_value - 1)))) - 1;
    const rep_t last_chunk_desired = ~(construct_bitmap_word(finish_chunk & (bits_per_value - 1)) - 1);

    if (start_word_idx == finish_word_idx) {
      set_sweep_bitmap(_sweep_bitmap_end[start_word_idx], first_chunk_desired & last_chunk_desired, set);
      set_sweep_bitmap(_sweep_bitmap_begin[start_word_idx], first_chunk_desired & last_chunk_desired, set);
      return;
    }

    set_sweep_bitmap(_sweep_bitmap_end[start_word_idx], first_chunk_desired, set);
    set_sweep_bitmap(_sweep_bitmap_begin[start_word_idx], first_chunk_desired, set);

    const rep_t word_to_write = set ? rep_t(-1) : 0;
    for (start_word_idx++; start_word_idx < finish_word_idx; start_word_idx++) {
      _sweep_bitmap_end[start_word_idx] = word_to_write;
      _sweep_bitmap_begin[start_word_idx] = word_to_write;
    }

    set_sweep_bitmap(_sweep_bitmap_end[finish_word_idx], last_chunk_desired, set);
    set_sweep_bitmap(_sweep_bitmap_begin[finish_word_idx], last_chunk_desired, set);
  }

  void mark_bitmap::set_sweep_bitmap_range(const std::size_t beg_word, const std::size_t end_word, const bool set_bit) {
    const std::size_t begin_chunk = beg_word >> (chunk_size_log_bits + value_log_bits);
    const std::size_t end_chunk = end_word >> (chunk_size_log_bits + value_log_bits);
    if (end_chunk - begin_chunk < 2) {
      //Nothing to do;
      return;
    }
    _set_sweep_bitmap_range(begin_chunk + 1, end_chunk - 1, set_bit);
  }

  void mark_bitmap::sweep2_phase(const bool set_bitmap) {
    assert(gc_handshake::process_struct->get_tolerate_sweep_chunk() == 0);
    std::size_t &i = gc_handshake::process_struct->get_tolerate_sweep_chunk();
    gc_control_block &cb = control_block();
    gc_allocator::globalListType &list = cb.global_free_list[gc_handshake::process_struct->global_list_index()];

    {
      Pre_sweep_list &pre_sweep_list = gc_handshake::process_struct->pre_sweep_list();
      assert(pre_sweep_list.size() % 2 == 0);
      while (!pre_sweep_list.empty()) {
        std::size_t beg_word = pre_sweep_list.front();
        pre_sweep_list.pop_front();
      
        std::size_t end_word = pre_sweep_list.front();
        pre_sweep_list.pop_front();

        put_to_global(list, beg_word, (end_word - beg_word) << 3);

        if (request_gc_termination) {
          return;
        }
        /* We can set all the bits within the chunk to be set as they will not
         * have any dirty chunk.
         */
        if (!is_marked(compute_bitmap_index(beg_word << 3), compute_bit_number(beg_word << 3))) {
          /* This is needed if, instead of the first word, we marked the second word because the
           * the first word was beginning of a dead object. Read the comment in expand_free_chunk().
           */
          beg_word++;
        }
        set_sweep_bitmap_range(beg_word, end_word - 1, set_bitmap);
      }
    }

    do {
      if (request_gc_termination) {
        break;
      }
      fetch_logical_chunk_to_process(i);
      if (i >= _total_logical_chunks) {
        break;
      }
      if (!is_end_sweep_bitmap_set(i, set_bitmap)) {
        process_logical_chunk(list, i, set_bitmap);
      }
    } while (true);
    //The following clearing of the other global allocator will not be required once we have the optimized sweep code.
    gc_allocator::globalListType &other_list = cb.global_free_list[1 - gc_handshake::process_struct->global_list_index()];
    for (uint8_t i = 0; i < gc_allocator::global_list_size(); i++) {
      other_list[i].store(gc_allocator::list_head());
      assert(other_list[i].load().empty());
    }
  }

  void mark_bitmap::_post_sweep_clear(atomic_rep_t &word, atomic_rep_t * bitmap_chunk,
                                      atomic_rep_t * other_bitmap_chunk, const bool set_bit) {
    rep_t val = word;
    bool is_other = other_bitmap_chunk != nullptr;
    rep_t iter = construct_bitmap_word(0);
    const std::size_t size = sizeof(atomic_rep_t) << chunk_size_log_bits;
    if (!set_bit) {
      val = ~val;
    }
    while (iter) {
      if (!(val & iter)) {
        std::memset(bitmap_chunk, 0x0, size);
        if (is_other) {
          std::memset(other_bitmap_chunk, 0x0, size);
        }
        set_sweep_bitmap(word, iter, set_bit);
      }
      iter >>= 1;
      bitmap_chunk += 1 << chunk_size_log_bits;
      if (is_other) {
        other_bitmap_chunk += 1 << chunk_size_log_bits;
      }
    }
  }

  void mark_bitmap::post_sweep_clear(const std::size_t nr_sweep_bitmap_word, const bool set_bit) {
    if (nr_sweep_bitmap_word >= _total_logical_chunks) {
      /* This is possible if a process terminates after finishing
       * sweep_phase2 but before fetching first bitmap word.
       */
      return;
    }
    assert(nr_sweep_bitmap_word < _sweep_bitmap_size);
    const std::size_t word_begin = nr_sweep_bitmap_word << (value_log_bits + chunk_size_log_bits);
    _post_sweep_clear(_sweep_bitmap_begin[nr_sweep_bitmap_word], _begin + word_begin, _weak + word_begin, set_bit);
    _post_sweep_clear(_sweep_bitmap_end[nr_sweep_bitmap_word], _end + word_begin, nullptr, set_bit);
  }

  void mark_bitmap::post_sweep_phase(per_process_struct *process_struct, const bool set_bit) {
    assert(gc_handshake::process_struct->get_tolerate_sweep_chunk() >= _total_logical_chunks);

    std::size_t &i = gc_handshake::process_struct->get_tolerate_sweep_chunk();
    do {
      if (request_gc_termination) {
        break;
      }
      fetch_sweep_bitmap_word_to_process(i);
      if (i >= _sweep_bitmap_size) {
        break;
      }
      post_sweep_clear(i, set_bit);
    } while (true);
  }

  static bool cleanup_sweep1_phase(per_process_struct *p, per_process_struct::liveness &expected) {
    per_process_struct::liveness desired = gc_handshake::process_struct->get_liveness();
    if (p->set_liveness(expected, desired)) {
      Pre_sweep_list &dead_list = p->pre_sweep_list();
      Pre_sweep_list &my_list = gc_handshake::process_struct->pre_sweep_list();

      /* If the size of dead_list is odd, that means the dead process couldn't
       * push both begin and end. In that case, simply get rid of the begin that
       * was pushed at the back.
       *
       * There are two scenarios in which the dead_list's size can be odd:
       * 1) If the process to which dead_list belong, died after adding the
       * beginning of the free range, but before adding the end of the range.
       * In this case, we must throw away the back.
       *
       * 2) If some process was cleaning up the dead_list, but died after popping
       * the beginning of a free range, but before popping the end. In this case,
       * The front of the deque will be -1. In this case, throw away the front.
       */
      if (dead_list.size() % 2 != 0) {
        if (dead_list.front() == std::size_t(-1)) {
          dead_list.pop_front();
        } else {
          dead_list.pop_back();
        }
      }

      /* In the loop below, there is a very short window wherein we might have
       * element next to the front one set to -1. At this point, if a process
       * dies, we need a way to recover out of it. So, we throw away the first
       * two elements.
       */
      if (dead_list.size() && dead_list[1] == std::size_t(-1)) {
        dead_list.pop_front();
        dead_list.pop_front();
      }

      while (!dead_list.empty()) {
        std::size_t first = dead_list[0];
        std::size_t second = dead_list[1];

        dead_list[1] = std::size_t(-1);
        //The popping of elements should happen only after -1 assignment above.
        std::atomic_signal_fence(std::memory_order_release);
        dead_list.pop_front();
        dead_list.pop_front();
        //The pushing of the two elements must happen only after popping above.
        std::atomic_signal_fence(std::memory_order_release);
        my_list.push_back(first);
        my_list.push_back(second);
      }

      expected.is_live = per_process_struct::Alive::Dead;
      bool ret = p->set_liveness(desired, expected);
      assert(ret);
      return true;
    }
    return false;
  }

  /*
   * Cleanup function called for a crashed process for recovery. After cleanup,
   * it restores the ownserhip of the dead process' structure that is taken
   * before the cleanup.
   */
  static bool cleanup_post_sweep_phase(per_process_struct *p, per_process_struct::liveness &expected, const bool set_bit) {
    per_process_struct::liveness desired = gc_handshake::process_struct->get_liveness();
    if (p->set_liveness(expected, desired)) {
      control_block().bitmap.post_sweep_clear(p->get_tolerate_sweep_chunk(), set_bit);
      p->reset_tolerate_sweep_chunk();
      expected.is_live = per_process_struct::Alive::Dead;
      bool assert_test = p->set_liveness(desired, expected);
      assert(assert_test);
      return true;
    }
    return false;
  }

  static void synchronize_gc_threads(const Barrier_indices n, Stage stage, const bool set_bit = 0) {
    gc_control_block &cb = control_block();
    uint16_t spin_count = 0;//We should may be try not initializing it, to incorporate some randomness.
    auto action_on_dead_process = [](per_process_struct *p) { p->mark_dead(); };

    inc_barrier(*gc_handshake::process_struct, n);

    versioned_pcount_t nr_live_process = cb.total_process_count;
    while(cb.barrier_sync[n] < nr_live_process.count && cb.stage == stage) {
      std::cpu_relax();
      if (++spin_count > 0) {
        nr_live_process = cb.total_process_count;
        continue;
      }
      pcount_t temp_live_process;
      switch(n) {
      case Barrier_indices::sync:
      case Barrier_indices::preMarking:
      case Barrier_indices::preSweep:
      case Barrier_indices::sweep2:
      case Barrier_indices::postSweep1:
        temp_live_process = cleanup_failures(action_on_dead_process,
                                             [](per_process_struct *p, per_process_struct::liveness &expected) -> bool {
            p->mark_dead();
            return true;
          });
        break;
      case Barrier_indices::sweep1:
        temp_live_process = cleanup_failures(action_on_dead_process, cleanup_sweep1_phase);
        break;
      case Barrier_indices::postSweep2:
        temp_live_process = cleanup_failures(action_on_dead_process, cleanup_post_sweep_phase, set_bit);
        break;
      default:
        /* marking1 and marking2 will not come here as they are related to marking and
         * dealt with separately in marking_phase() function.
         */
        assert(false);
      }
      if (request_gc_termination) {
        return;
      }
      if (temp_live_process > 0 &&
          cb.total_process_count.compare_exchange_strong(nr_live_process,
                                                         versioned_pcount_t(temp_live_process,
                                                                            nr_live_process.version + 1))) {
        nr_live_process.count = temp_live_process;
        nr_live_process.version++;
      }
    }

    gc_handshake::process_struct->reset_barrier_info(next_barrier_index_mapping[n]);
  }

  /*
   * Asserts during sweep signal that the new allocator list is completely empty.
   */
  void assert_current_alloc_list_empty() {
    gc_allocator::globalListType &list =
          control_block().global_free_list[1 - gc_handshake::thread_struct_handles.handle->status_idx.load().index()];
    for (uint8_t i = 0; i < gc_allocator::global_list_size(); i++) {
      assert(list[i].load().empty());
    }
  }


  void trace_gc_cycle(int n, const gc_control_block &cb) {
    static const bool tracep = ruts::env_flag("GC_TRACE_CYCLES");
    if (tracep) {
      std::string suffix
        = (n%10==1 && n%100!=11) ? "st"
        : (n%10==2 && n%100!=12) ? "nd"
        : (n%10==3 && n%100!=13) ? "rd"
        : "th";
      const gc_mem_stats &ms = cb.mem_stats;
      std::cout << "Processing " << n << suffix << " iteration"
                << " [" << ms.cycle_number() << ": "
                << ms.bytes_in_use() << " bytes marked of "
                << ms.bytes_in_heap()
                << ", " << ms.n_processes() << " process"
                << (ms.n_processes()==1 ? "" : "es")
                << "]" << std::endl;
    }
  }
  
  void start_gc(Stage local_stage) {
    gc_control_block &cb = control_block();
    int count = 0;
    std::size_t gc_cycle_num = cb.mem_stats.cycle_number();
    gc_status local_status = gc_handshake::process_struct->get_gc_status();

    //Following switch-case is to fix the barrier info to contain right barrier index.
    switch (local_stage) {
    case Stage::Sweeped:
    case Stage::preTracing:
      gc_handshake::process_struct->reset_barrier_info(Barrier_indices::sync);
      break;
    case Stage::Tracing:
      gc_handshake::process_struct->reset_barrier_info(Barrier_indices::preMarking);
      break;
    case Stage::Traced:
    case Stage::preSweeping:
      gc_handshake::process_struct->reset_barrier_info(Barrier_indices::preSweep);
      break;
    case Stage::Sweeping:
      gc_handshake::process_struct->reset_barrier_info(Barrier_indices::sweep1);
    }

    while (true) {
      trace_gc_cycle(count, cb);
      switch (local_stage) {
      case Stage::Sweeped: {
        local_status.status_idx.status = gc_handshake::Signum::sigSweep;
        if (cb.status.compare_exchange_strong(local_status,
                                              gc_status(gc_handshake::Signum::sigSync1,
                                              local_status.status_idx.idx))) {
          local_status.status_idx.status = gc_handshake::Signum::sigSync1;
        }
        gc_handshake::process_struct->set_gc_status(local_status.data);
        assert(gc_handshake::process_struct->get_gc_status() == cb.status.load().data);

        gc_handshake::handshake(gc_handshake::Signum::sigSync1);
        if (request_gc_termination) {
          break;
        }
        //barriers used for marking can be cleaned at this point
        cb.marking_barrier = marking_barrier_type(Barrier_stage::incrementing, Barrier_indices::marking1);
        cb.stage.compare_exchange_strong(local_stage, Stage::preTracing);
        local_stage = Stage::preTracing;
      }
      case Stage::preTracing: {
        //All GC threads must synchronize at this point.
        synchronize_gc_threads(Barrier_indices::sync, local_stage);
        if (request_gc_termination) {
          break;
        }
        cb.barrier_sync[Barrier_indices::preSweep] = 0;
        cb.barrier_sync[Barrier_indices::sweep1] = 0;

        local_status.status_idx.status = gc_handshake::Signum::sigSync1;
        if (cb.status.compare_exchange_strong(local_status,
                                              gc_status(gc_handshake::Signum::sigSync2,
                                              local_status.status_idx.idx))) {
          local_status.status_idx.status = gc_handshake::Signum::sigSync2;
        }
        gc_handshake::process_struct->set_gc_status(local_status.data);
        assert(gc_handshake::process_struct->get_gc_status() == cb.status.load().data);

        gc_handshake::handshake(gc_handshake::Signum::sigSync2);
        if (request_gc_termination) {
          break;
        }
        //Enable mutators to sync in optimized manner with gc thread.
        gc_handshake::process_struct->gc_mutator_weak_sync = 0;
        cb.stage.compare_exchange_strong(local_stage, Stage::Tracing);
        local_stage = Stage::Tracing;
      }
      case Stage::Tracing: {
        //All GC threads must synchronize at this point.
        synchronize_gc_threads(Barrier_indices::preMarking, local_stage);
        if (request_gc_termination) {
          break;
        }
        cb.barrier_sync[Barrier_indices::sweep2] = 0;
        cb.barrier_sync[Barrier_indices::postSweep1] = 0;
        cb.barrier_sync[Barrier_indices::postSweep2] = 0;
        cb.bitmap.reset_logical_chunk_count();

        local_status.status_idx.status = gc_handshake::Signum::sigSync2;
        if (cb.status.compare_exchange_strong(local_status,
                                              gc_status(gc_handshake::Signum::sigAsync,
                                                        local_status.status_idx.idx))) {
          local_status.status_idx.status = gc_handshake::Signum::sigAsync;
        }
        gc_handshake::process_struct->set_gc_status(local_status.data);
        assert(gc_handshake::process_struct->get_gc_status() == cb.status.load().data);

        constexpr auto stage_bits_fld = bits::field<Weak_stage, uint16_t>(0, 2);
        uint16_t expected = stage_bits_fld.encode(Weak_stage::Normal);
        uint16_t desired = stage_bits_fld.encode(Weak_stage::Trace);
        if (!cb.weak_stage.compare_exchange_strong(expected, desired)) {
          assert(stage_bits_fld.decode(expected) == Weak_stage::Trace);
        }

        gc_handshake::post_handshake(gc_handshake::Signum::sigAsync, true);
        capture_global_roots(cb, gc_handshake::process_struct->traversal_queue());
        gc_handshake::wait_handshake(gc_handshake::Signum::sigAsync, true);
        if (request_gc_termination) {
          break;
        }

        marking_phase(*gc_handshake::process_struct);
        if (request_gc_termination) {
          break;
        }
        cb.barrier_sync[Barrier_indices::sync] = 0;
        //No synchronization required at this point as already done in marking_phase(process_struct)

        cb.stage.compare_exchange_strong(local_stage, Stage::Traced);
        local_stage = Stage::Traced;
      }
      case Stage::Traced: {
        local_status.status_idx.status = gc_handshake::Signum::sigAsync;
        if (cb.status.compare_exchange_strong(local_status,
                                              gc_status(gc_handshake::Signum::sigSweep,
                                                        1 - local_status.status_idx.idx))) {
          local_status = gc_status(gc_handshake::Signum::sigSweep, 1 - local_status.status_idx.idx);
        }
        gc_handshake::process_struct->set_gc_status(local_status.data);
        assert(gc_handshake::process_struct->get_gc_status() == cb.status.load().data);

        gc_handshake::post_handshake(gc_handshake::Signum::sigSweep, true);
        adjust_inbound_weak_pointers(cb);
        gc_handshake::wait_handshake(gc_handshake::Signum::sigSweep, true);
        if (request_gc_termination) {
          break;
        }

        cb.stage.compare_exchange_strong(local_stage, Stage::preSweeping);
        local_stage = Stage::preSweeping;
      }
      case Stage::preSweeping: {
        //All GC threads must synchronize at this point.
        synchronize_gc_threads(Barrier_indices::preSweep, local_stage);
        if (request_gc_termination) {
          break;
        }
        cb.barrier_sync[Barrier_indices::preMarking] = 0;

        sweep1_phase();

        cb.stage.compare_exchange_strong(local_stage, Stage::Sweeping);
        local_stage = Stage::Sweeping;
      }
      case Stage::Sweeping: {
        synchronize_gc_threads(Barrier_indices::sweep1, local_stage);
        if (request_gc_termination) {
          break;
        }

        cb.bitmap.sweep2_phase(local_status.status_idx.idx);
        if (request_gc_termination) {
          break;
        }

        synchronize_gc_threads(Barrier_indices::sweep2, local_stage, local_status.status_idx.idx);
        if (request_gc_termination) {
          break;
        }

        post_sweep_weak_ptr_sync(cb, gc_handshake::process_struct);
        if (request_gc_termination) {
          break;
        }

        synchronize_gc_threads(Barrier_indices::postSweep1, local_stage, local_status.status_idx.idx);
        if (request_gc_termination) {
          break;
        }

        cb.bitmap.post_sweep_phase(gc_handshake::process_struct, local_status.status_idx.idx);
        if (request_gc_termination) {
          break;
        }

        synchronize_gc_threads(Barrier_indices::postSweep2, local_stage, local_status.status_idx.idx);
        if (request_gc_termination) {
          break;
        }
        gc_handshake::process_struct->reset_tolerate_sweep_chunk();
        cb.bitmap.test_bitmaps(local_status.status_idx.idx);

        cb.stage.compare_exchange_strong(local_stage, Stage::Sweeped);
        local_stage = Stage::Sweeped;

        gc_handshake::thread_struct_list.deletion(gc_handshake::in_memory_thread_struct::is_marked);
        cb.process_struct_list.deletion(gc_handshake::process_struct, per_process_struct::is_marked);
        gc_handshake::process_struct->clear();
      }
      } //switch-case statement

      if (request_gc_termination) {
        {
          std::lock_guard<std::mutex> lk(gc_termination_mutex);
          request_gc_termination = false;
        }
        gc_terminated.notify_all();
        return;
      }
      count++;
      // This may not be the appropriate place to put this.  We want
      // it at a point where nobody's marking yet and everybody's
      // finished marking.
      gc_cycle_num = cb.mem_stats.inc_cycle_num_to(gc_cycle_num+1);
    } //while(true)
  }

  /*
   * atexit handler. Helps in GC to gracefully terminate when process
   * termination is requested.
   */
  void atexit_gc_handler() {
    request_gc_termination = true;
    std::unique_lock<std::mutex> lk(gc_termination_mutex);
    gc_terminated.wait(lk, []{return !request_gc_termination;});
  }
}

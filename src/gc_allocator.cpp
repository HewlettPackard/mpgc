/*
 *
 *  Multi Process Garbage Collector
 *  Copyright Â© 2016 Hewlett Packard Enterprise Development Company LP.
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Lesser General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the 
 *  Application containing code generated by the Library and added to the 
 *  Application during this compilation process under terms of your choice, 
 *  provided you also meet the terms and conditions of the Application license.
 *
 */

/*
 * gc_allocator.cpp
 *
 *  Created on: Jul 22, 2015
 *      Author: gidra
 */

#include <cstdlib>
#include <cassert>
#include <mutex>
#include <iostream>
#include <atomic>

#include "mpgc/gc_allocator.h"
#include "mpgc/gc_handshake.h"

namespace mpgc {
  extern void global_allocation_epilogue();

  uint8_t gc_allocator::_global_list_size = 0;
  gc_allocator::globalListType *gc_allocator::global_free_lists = nullptr;

    void gc_allocator::_put_to_global(globalListType &list, const std::size_t idx, offset_ptr<global_chunk> c) {
      assert(c);
      list_head temp = list[idx];
      list_head desired(c);
      do {
        desired._version = temp._version + 1;
        c->set_next(temp._ptr);
      } while(!list[idx].compare_exchange_weak(temp, desired));
    }

    offset_ptr<gc_allocator::global_chunk> gc_allocator::get_chunk_from_global(globalListType &list, const std::size_t idx) {
      list_head cur = list[idx];
      while (cur._ptr && !list[idx].compare_exchange_weak(cur, list_head(cur._ptr->next(), cur._version + 1)));//end of loop
      return cur._ptr;
    }

    offset_ptr<gc_allocator::global_chunk> gc_allocator::_get_from_global(globalListType &list,
                                        const std::size_t size,
                                        const std::size_t idx) {
      for (std::size_t i = idx; i < _global_list_size; i++) {
        offset_ptr<global_chunk> c = get_chunk_from_global(list, i);

        if (c) {//It's possible that somebody else fetched the last chunk before us.
          if (c->size() < size) { //The condition can be true only the first time.
            //Not large enough. Put it back!
            _put_to_global(list, i, c);
          } else {
            //Found a big enough chunk. If the requested size is smaller than a slab_size,
            //then we chop that much and put back the rest.
            std::size_t size_to_chop = 0;
            if (size > slab_size) {
              size_to_chop = size;
            } else if (c->size() > slab_size) {
              size_to_chop = slab_size;
            }

            const std::size_t new_chunk_size = c->size() - size_to_chop;
            if (size_to_chop > 0 && new_chunk_size >= min_global_chunk_size()) {
              /*
               * It is important to set the size of the leftover chunk first and then of
               * the chunk that will be used by the local allocator for fault-tolerance.
               * Only after setting these two values should the put_to_global be called.
               * This is necessary because otherwise, process failure in the middle of
               * these operations can lead to a deadlock during sweeping while cleaning
               * the garbage gc_descriptors.
               */
              global_chunk* put_back = new (reinterpret_cast<uint8_t*>(c.as_bare_pointer()) + size_to_chop) global_chunk(new_chunk_size);
              c->set_size(size_to_chop);
              put_to_global(list, new_chunk_size, put_back);
            }
            return c;
          }
        }
      }
      return nullptr;
    }

    offset_ptr<gc_allocator::global_chunk> gc_allocator::get_from_global(const std::size_t size) {
      const std::size_t idx = global_list_index_for(size);
      do {
        global_allocation_epilogue();
        /* This while loop will ensure that we don't end-up in a situation where some other
         * thread holds the entire memory chunk for its own allocation purpose, and hence
         * this thread fails.
         * However, once we have a working GC, the thread doesn't have to fail, it can go
         * and start helping the GC until it can reclaim the required free space.
         */
        offset_ptr<global_chunk> c = _get_from_global(global_free_lists[gc_handshake::thread_struct_handles.handle->status_idx.load().index()], size, idx);
        if (c) {
          return c;
        }
      } while (true);
      return nullptr;
    }

    void* gc_allocator::alloc(std::size_t size) {
      local_chunk *chunk;
      std::size_t leftover_size = 0;
      size = align_size_up(size, sizeof(std::size_t));

      localPoolType &local_chunks = gc_handshake::thread_struct_handles.handle->local_free_list;
      localPoolType::iterator it = local_chunks.lower_bound(size);

      if (it == local_chunks.end()) {
        //We don't have a big enough chunk
        offset_ptr<global_chunk> c = get_from_global(size);
        leftover_size = c->size() - size;
        assert(c->size() >= size);
        chunk = reinterpret_cast<local_chunk*>(c.as_bare_pointer());
      } else {
        chunk = it->second;
        leftover_size = it->first - size;
        if (chunk->next()) {
          it->second = chunk->next();
        } else {
          local_chunks.erase(it);
        }
      }
      uint8_t *return_addr = reinterpret_cast<uint8_t*>(chunk);
      if (leftover_size >= sizeof(local_chunk)) {
        local_chunk*& temp = local_chunks[leftover_size];
        temp = new (return_addr + size) local_chunk(leftover_size, temp);
      } else if (leftover_size){
        *reinterpret_cast<std::size_t*>(return_addr + size) = leftover_size;
      }
      /*
       * It is essential to keep the size of object in the first word until it
       * gets initialized with a gc_descriptor in the allocation_epilogue function
       * for fault-tolerance. If we clean it in the following memset, and the process
       * crashes after that but before gc_descriptor construction, then the garbage
       * gc_descriptor cleanup during sweep can get into a infinite-loop assuming the
       * size to be 0.
       */
      *reinterpret_cast<std::size_t*>(return_addr) = size;
      //Zero-out the memory
      std::memset(return_addr + sizeof(std::size_t), 0x0, size - sizeof(std::size_t));
      return chunk;
    }
}

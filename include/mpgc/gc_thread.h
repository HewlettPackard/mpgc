/*
 *
 *  Multi Process Garbage Collector
 *  Copyright Â© 2016 Hewlett Packard Enterprise Development Company LP.
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Lesser General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the 
 *  Application containing code generated by the Library and added to the 
 *  Application during this compilation process under terms of your choice, 
 *  provided you also meet the terms and conditions of the Application license.
 *
 */

/*
 * gc_thread.h
 *
 *  Created on: Jul 22, 2015
 *      Author: gidra
 */

#ifndef GC_GC_THREAD_H_
#define GC_GC_THREAD_H_

#include <sys/types.h>
#include <unistd.h>

#include<deque>
#include<cassert>
#include<cstdio>
#include<iomanip>
#include<cstring>
#include<string>
#include<random>

#include "ruts/util.h"
#include "ruts/runtime_array.h"
#include "ruts/managed.h"
#include "ruts/collections.h"
#include "ruts/atomic16B.h"
#include "mpgc/work_stealing_wq.h"
#include "mpgc/gc_allocated.h"
#include "mpgc/gc_desc.h"
#include "mpgc/mark_buffer.h"
#include "mpgc/offset_ptr.h"
#include "mpgc/gc_skiplist_allocator.h"
/*
 * This class contains all the per-process structures.
 */
namespace mpgc {
  namespace gc_handshake {
    enum class Signum : char;
  }

  enum class Stage : unsigned char;

  struct status_global_list_idx {
    gc_handshake::Signum status;
    uint8_t idx;
    status_global_list_idx(gc_handshake::Signum s, uint8_t idx = 0) : status(s), idx(idx) {}
  };
  union gc_status {
    status_global_list_idx status_idx;
    uint16_t data;
    gc_status(gc_handshake::Signum s) : status_idx(status_global_list_idx(s)) {}
    gc_status(gc_handshake::Signum s, uint8_t idx) : status_idx(status_global_list_idx(s, idx)) {}
    gc_status(uint16_t d) : data(d) {}
    gc_status() : data(0) {}

    gc_handshake::Signum status() { return status_idx.status; }
    uint8_t index() { return status_idx.idx; }
  };

  enum Barrier_indices : int8_t {
    undef = -1,
    sync = 0,
    preMarking,
    preSweep,
    sweep1,
    sweep2,
    postSweep1,
    postSweep2,
    arraysize,
    marking1 = arraysize,
    marking2, //must be the last one
  };

  using Mbuf = mark_buffer<offset_ptr<const gc_allocated>>;
  struct mutator_persist {
    Mbuf mbuf;
    chunk_expansion_slot expansion_slot;
    gc_allocator::slot_number slot;

    static bool is_marked(mutator_persist *b) {
      return Mbuf::is_marked(&b->mbuf);
    }
    mutator_persist() : mbuf(), slot() {}
    ~mutator_persist();
  };

  using Mpersist = mutator_persist;
  using Mutator_persist_list = ruts::sequential_lazy_delete_collection<mutator_persist, ruts::managed_space::allocator<mutator_persist>>;
  using Traversal_queue = work_stealing_wq<offset_ptr<const gc_allocated>>;

  using pcount_t = uint16_t;//any variable which needs to hold process count must use this.

  /*
   * It is very important to have incrementing = 0. This is
   * required because in the Barrier_info below, Barrier_stage
   * shares the same space as the dummy in marking_barrier, which
   * will always contain a 0.
   */
  enum class Barrier_stage : int8_t {
    unincremented = -1,
    incrementing = 0,
    incremented
  };

  union marking_barrier_type {
    struct {
      uint32_t version = 0;
      pcount_t barrier = 0;
      Barrier_indices index = Barrier_indices::undef;
      Barrier_stage bstage = Barrier_stage::unincremented;
    } _info;
    uint64_t _data;

    marking_barrier_type() noexcept : _info() {}
    marking_barrier_type(Barrier_stage s, Barrier_indices i) noexcept : _data(0) {
      _info.index = i;
      _info.bstage = s;
    }

    marking_barrier_type(const marking_barrier_type &rhs) noexcept = default;
    marking_barrier_type(marking_barrier_type &&rhs) noexcept = default;
    // marking_barrier_type(const marking_barrier_type &rhs) noexcept : _data(rhs._data) {}
    // marking_barrier_type(marking_barrier_type &&rhs) noexcept : _data(std::move(rhs._data)) {}

    marking_barrier_type &operator =(const marking_barrier_type &rhs) noexcept = default;
    marking_barrier_type &operator =(marking_barrier_type &&rhs) noexcept = default;    // marking_barrier_type &operator =(const marking_barrier_type &rhs) noexcept {
    //   _data = rhs._data;
    //   return *this;
    // }
    // marking_barrier_type &operator =(marking_barrier_type &&rhs) noexcept {
    //   _data = std::move(rhs._data);
    //   return *this;
    // }
  };

  enum struct gc_mutator_weak_sync_state {
    Work = 0,
    Stop = 1
  };

  class per_process_struct {
  public:
    enum class Alive : uint32_t {
      Live,
      Dead
    };

    struct alignas(16) liveness {
      unsigned long long creation_time;
      pid_t pid;
      Alive is_live;

      liveness() : creation_time(0), pid(0), is_live(Alive::Live) {}
      liveness(pid_t p) : pid(p), is_live(Alive::Live) {
        creation_time = get_creation_time(pid);
      }
      liveness(pid_t p, unsigned long long t) : creation_time(t), pid(p), is_live(Alive::Dead) {}
      liveness(pid_t p, unsigned long long t, Alive live) : creation_time(t), pid(p), is_live(live) {}
    };

    using Barrier_info = marking_barrier_type;

  private:
    union {
      chunk_expansion_slot sweep1_data;
      std::size_t sweep_nr_chunk;
      offset_ptr<const gc_allocated> marking_obj_ref;
    };

    ruts::atomic16B<liveness> _liveness;
  public:
    std::mt19937              rand;
  private:
    Barrier_info              _binfo;
    Mutator_persist_list          _mutator_persist_list;

    Traversal_queue           _tqueue;

    volatile gc_status        _status;
  public:

    std::atomic<uint16_t> gc_mutator_weak_sync;
    volatile bool        sweep1_enabled;

    chunk_expansion_slot& get_sweep1_data() { return sweep1_data;}
    chunk_expansion_slot* sweep1_data_ptr() { return &sweep1_data;}

   per_process_struct () :
      sweep1_data(),
      _liveness(liveness(getpid())),
      rand(_liveness.load().creation_time),
      _tqueue(),
      sweep1_enabled(false)
    {
      static_assert(sizeof(liveness) <= 16, "Liveness object must be at least 16 bytes long.");
    }

    ~per_process_struct() {
      _tqueue.~Traversal_queue();
      _mutator_persist_list.~Mutator_persist_list();
    }

    void mark_dead() {
      liveness temp = _liveness;
      temp.is_live = Alive::Dead;
      _liveness = temp;
    }

    bool mark_dead(liveness expected) {
      liveness desired = expected;
      desired.is_live = Alive::Dead;
      //false may be returned only if someone has taken ownership and is working on it (or has died while doing so).
      return _liveness.compare_exchange_strong(expected, desired) || expected.is_live == Alive::Dead;
    }

    static bool is_marked(per_process_struct *p) {
      return p->_liveness.load().is_live == Alive::Dead;
    }
    bool steal(Traversal_queue &other) {
      return _tqueue.steal(other);
    }

    void set_marking_ref(offset_ptr<const gc_allocated> ref) {
      assert(!marking_obj_ref);
      marking_obj_ref = ref;
    }
    offset_ptr<const gc_allocated>& marking_ref() {
      return marking_obj_ref;
    }
    void clear_marking_ref() {
      marking_obj_ref = nullptr;
    }

    void reset_tolerate_sweep_chunk() {
      sweep_nr_chunk = 0;
    }

    std::size_t& get_tolerate_sweep_chunk() {
      return sweep_nr_chunk;
    }

    Barrier_info get_barrier_info() {
      return _binfo;
    }

    pcount_t &barrier_id_ref() {
      return _binfo._info.barrier;
    }

    pcount_t get_barrier_id() {
      return _binfo._info.barrier;
    }

    uint32_t get_barrier_version() {
      return _binfo._info.version;
    }

    uint32_t &barrier_version_ref() {
      return _binfo._info.version;
    }

    marking_barrier_type &marking_barrier_ref() {
      return _binfo;
    }

    marking_barrier_type get_marking_barrier() {
      return _binfo;
    }

    void set_barrier_incrementing() {
      _binfo._info.bstage = Barrier_stage::incrementing;
    }

    void set_barrier_unincremented() {
      _binfo._info.bstage = Barrier_stage::unincremented;
    }

    void set_barrier_incremented() {
      _binfo._info.bstage = Barrier_stage::incremented;
    }

    Barrier_stage get_barrier_stage() {
      return _binfo._info.bstage;
    }

    Barrier_indices get_barrier_index() {
      return _binfo._info.index;
    }

    void reset_barrier_info(Barrier_indices barrier_idx) {
      Barrier_info temp;
      temp._info.index = barrier_idx;
      if (barrier_idx == Barrier_indices::marking1 || barrier_idx == Barrier_indices::marking2) {
        temp._info.version = _binfo._info.version;
      }
      _binfo = temp;
    }

    const uint8_t global_list_index() { return _status.status_idx.idx;}
    const gc_handshake::Signum status() { return _status.status_idx.status;}
    const uint16_t get_gc_status() { return _status.data;}

    void set_gc_status(uint16_t d) { _status.data = d;}
    void set_status(gc_handshake::Signum s) { _status.status_idx.status = s;}

    Mutator_persist_list &mutator_persist_list() {
      return _mutator_persist_list;
    }

    Traversal_queue &traversal_queue() {
      return _tqueue;
    }

    void clear() {
      _mutator_persist_list.deletion(mutator_persist::is_marked);
    }

    liveness get_liveness() { return _liveness.load();}

    bool set_liveness(liveness expected, liveness desired) {
      return _liveness.compare_exchange_strong(expected, desired);
    }

    static unsigned long long get_creation_time(pid_t pid) {
      std::FILE *file = std::fopen(("/proc/" + std::to_string(pid) + "/stat").c_str(), "r");
      if (file) {
        unsigned long long start_time;
        std::fscanf(file, "%*d %*s %*c %*d %*d %*d %*d %*d %*u %*u %*u %*u %*u %*u %*u %*d %*d %*d %*d %*d %*d %llu", &start_time);
        std::fclose(file);
        return start_time;
      }
      return -1;
    }
  };

  class mark_bitmap {
    using rep_t = std::size_t;
    using atomic_rep_t = std::atomic<rep_t>;
    using bitmap_idx_t = std::size_t;
    using bit_number_t = std::size_t;
    using bitmap_type = ruts::runtime_array<atomic_rep_t, ruts::managed_space::allocator<atomic_rep_t>>;
    using Allocator = ruts::managed_space::allocator<atomic_rep_t>;

    static constexpr uint8_t bits_per_value = sizeof(rep_t) * 8;
    static constexpr uint8_t value_log_bits = 6;
    static constexpr uint8_t chunk_size_log_bits = 10;

    const std::size_t _size;
    const std::size_t _total_logical_chunks;
    const std::size_t _sweep_bitmap_size;
    Allocator _alloc;

    atomic_rep_t * const _begin;
    atomic_rep_t * const _end;
    atomic_rep_t * const _weak;

    /* Sweep bitmap is a bitmap over the mark-bitmap. It is required
     * in order to have a lock-free sweep function. There is a bit
     * field for each logical chunk of the mark-bitmap, both in begin
     * and end. A logical chunk is a single entity swept by a GC thread
     * at a time. In other words, it is the smallest entity that a GC
     * thread sweeps in one instance.
     *
     * The meaning of 0s and 1s is toggled after every GC cycle.
     */
    atomic_rep_t * const _sweep_bitmap_begin;
    atomic_rep_t * const _sweep_bitmap_end;

    std::atomic<std::size_t> _logical_chunks;
    std::atomic<std::size_t> _sweep_bitmap_words;

    void fetch_logical_chunk_to_process(std::size_t &i) {
      i = _logical_chunks;
      while (!_logical_chunks.compare_exchange_weak(i, i + 1));
    }

    void fetch_sweep_bitmap_word_to_process(std::size_t &i) {
      i = _sweep_bitmap_words;
      while(!_sweep_bitmap_words.compare_exchange_weak(i, i + 1));
    }

    static constexpr rep_t construct_left_mask(const bit_number_t bit) {
      return rep_t(-1) >> bit;
    }

    static constexpr rep_t construct_right_mask(const bit_number_t bit) {
      return rep_t(-1) << ((bits_per_value - 1) - bit);
    }

    static constexpr rep_t construct_bitmap_word(const bit_number_t bit) {
      return rep_t(1) << ((bits_per_value - 1) - bit);
    }

    static constexpr bitmap_idx_t compute_bitmap_index(const std::size_t word_offset) {
      return word_offset >> value_log_bits;
    }

    static bit_number_t compute_bit_number(const std::size_t word_offset) {
      return word_offset & (bits_per_value - 1);
    }

    void set_sweep_bitmap(atomic_rep_t &word, const rep_t desired, const bool set) {
      set ? word.fetch_or(desired) : word.fetch_and(~desired);
    }

    void set_sweep_bitmap_begin(const std::size_t nr_chunk, const bool set) {
      set_sweep_bitmap(_sweep_bitmap_begin[nr_chunk >> value_log_bits],
                       construct_bitmap_word(nr_chunk & (bits_per_value - 1)),
                       set);
    }

    void set_sweep_bitmap_end(const std::size_t nr_chunk, const bool set) {
      set_sweep_bitmap(_sweep_bitmap_end[nr_chunk >> value_log_bits],
                       construct_bitmap_word(nr_chunk & (bits_per_value - 1)),
                       set);
    }

    void set_sweep_bitmap_both(const std::size_t nr_chunk, const bool set) {
      const std::size_t index = nr_chunk >> value_log_bits;
      const std::size_t desired = construct_bitmap_word(nr_chunk & (bits_per_value - 1));
      set_sweep_bitmap(_sweep_bitmap_end[index], desired, set);
      set_sweep_bitmap(_sweep_bitmap_begin[index], desired, set);
    }

    bool is_end_sweep_bitmap_set(const std::size_t nr_chunk, const bool set) {
      const rep_t val = _sweep_bitmap_end[nr_chunk >> value_log_bits];
      const rep_t expected = construct_bitmap_word(nr_chunk & (bits_per_value - 1));
      return set ? val & expected : ~val & expected;
    }

    void _post_sweep_clear(atomic_rep_t &, atomic_rep_t *, atomic_rep_t *, const bool);
    void _set_sweep_bitmap_range(const std::size_t, const std::size_t, const bool);

    static std::size_t compute_bitmap_size(std::size_t heap_size) {
      //The count we receive is in bytes.
      //3 bits for bytes per word, and n bits for bits per rep_t.
      //Each bit represents a word in the heap.
      assert(!(heap_size & ((rep_t(1) << (value_log_bits + 3)) - 1)));
      return compute_bitmap_index(heap_size >> 3);
    }

    atomic_rep_t &lookup_begin(const bitmap_idx_t idx) {
      assert(idx < _size);
      return _begin[idx];
    }

    atomic_rep_t &lookup_end(const bitmap_idx_t idx) {
      assert(idx < _size);
      return _end[idx];
    }

    atomic_rep_t &lookup_weak(const bitmap_idx_t idx) {
      assert(idx < _size);
      return _weak[idx];
    }

    bool mark_begin(const bitmap_idx_t idx, const bit_number_t bit) {
      atomic_rep_t &B = lookup_begin(idx);
      rep_t desired = construct_bitmap_word(bit);
      const rep_t res = B.fetch_or(desired);
      return !(res & desired);
    }

    bool mark_end(const bitmap_idx_t idx, const bit_number_t bit) {
      atomic_rep_t &B = lookup_end(idx);
      rep_t desired = construct_bitmap_word(bit);
      const rep_t res = B.fetch_or(desired);
      return !(res & desired);
    }

    bool unmark_end(const std::size_t word) {
      atomic_rep_t &B = lookup_end(compute_bitmap_index(word));
      rep_t desired = construct_bitmap_word(compute_bit_number(word));
      const rep_t prev = B.fetch_and(~desired);
      return prev & desired;
    }

    void mark_weak(const std::size_t word) {
      atomic_rep_t &B = lookup_weak(compute_bitmap_index(word));
      rep_t desired = construct_bitmap_word(compute_bit_number(word));
      B.fetch_or(desired);
    }

    bool is_end_marked(const bitmap_idx_t idx, const bit_number_t bit) {
      return lookup_end(idx) & construct_bitmap_word(bit) ? true : false;
    }

    bool _mark_end_first(const std::size_t beg_word, const std::size_t end_word) {
      mark_end(compute_bitmap_index(end_word), compute_bit_number(end_word));
      if (mark_begin(compute_bitmap_index(beg_word), compute_bit_number(beg_word))) {
        return true;
      }
      return false;
    }

    bool _mark_begin_first(const std::size_t beg_word, const std::size_t end_word) {
      mark_begin(compute_bitmap_index(beg_word), compute_bit_number(beg_word));
      if (mark_end(compute_bitmap_index(end_word), compute_bit_number(end_word))) {
        return true;
      }
      return false;
    }

    void mark_begin_first(const std::size_t beg_word, const std::size_t end_word) {
      if (mark_begin(compute_bitmap_index(beg_word), compute_bit_number(beg_word))) {
        mark_end(compute_bitmap_index(end_word), compute_bit_number(end_word));
      }
    }

    bool is_marked(const bitmap_idx_t idx, const bit_number_t bit) {
      return lookup_begin(idx) & construct_bitmap_word(bit);
    }

    /* Let us keep it simple for now. We want every entry in the sweep_bitmap
     * to cover 1K of rep_t mark_bitmap entries (8K memory).
     * TODO: This function needs to be more sophisticated in the way we deal
     * with smaller heap sizes etc.
     */
    static constexpr std::size_t compute_logical_chunk_count(std::size_t s) {
      return s >> chunk_size_log_bits;
    }

    // Input is number of logical chunks of mark-bitmap.
    static constexpr std::size_t compute_sweep_bitmap_size(std::size_t s) {
      return s >> value_log_bits;
    }

    void clear_chunk_begin(const std::size_t nr_chunk) {
      std::memset(_begin + (nr_chunk << chunk_size_log_bits), 0x0, sizeof(atomic_rep_t) << chunk_size_log_bits);
    }

    void clear_chunk_end(const std::size_t nr_chunk) {
      std::memset(_end + (nr_chunk << chunk_size_log_bits), 0x0, sizeof(atomic_rep_t) << chunk_size_log_bits);
    }

  public:

    static std::size_t compute_total_bitmap_size(const std::size_t heap_size) {
      std::size_t bitmap_size = compute_bitmap_size(heap_size);
      std::size_t sweep_bitmap_size = compute_sweep_bitmap_size(compute_logical_chunk_count(bitmap_size));
      return sizeof(atomic_rep_t) * (bitmap_size * 3 + sweep_bitmap_size * 2);
    }

    mark_bitmap(std::size_t heap_size, const Allocator &alloc = Allocator()) :
                                         _size(compute_bitmap_size(heap_size)),
                                         _total_logical_chunks(compute_logical_chunk_count(_size)),
                                         _sweep_bitmap_size(compute_sweep_bitmap_size(_total_logical_chunks)),
                                         _alloc(alloc),
                                         _begin(_alloc.allocate(3 * _size + 2 * _sweep_bitmap_size)),
                                         _end(_begin + _size),
                                         _weak(_end + _size),
                                         _sweep_bitmap_begin(_weak + _size),
                                         _sweep_bitmap_end(_sweep_bitmap_begin + _sweep_bitmap_size),
                                         _logical_chunks(0),
                                         _sweep_bitmap_words(0)
  {
      /* TODO: This is too much of work. We have to come up with a way where we don't need to
       * clear all the bitmaps because we can come up with a solution where the bitmaps are
       * allocated on a new file which is already zero-initialized.
       */
      std::memset(_begin, 0x0, 3 * sizeof(atomic_rep_t) * _size +
                               2 * sizeof(atomic_rep_t) * _sweep_bitmap_size);
  }

    ~mark_bitmap() {
      _alloc.deallocate(_begin, 1);
    }

    void clear() {
      std::memset(_begin, 0x0, _size * sizeof(atomic_rep_t));
      std::memset(_end,   0x0, _size * sizeof(atomic_rep_t));
      std::memset(_weak,  0x0, _size * sizeof(atomic_rep_t));
    }

    void print() {
      ruts::reset_flags_on_exit reset(std::cout);
      std::cout << std::setfill('0') << std::hex;
      for (bitmap_idx_t i = 0; i < _size;) {
        std::cout << "[" << std::setw(3) << i << "]";
        std::cout << std::setw(16) << _begin[i] << ":" << std::setw(16) << _end[i];
        std::cout << "\t";
        i++;
        if (i % 4 == 0) {
          std::cout << "\n";
        }
      }
    }

    bool is_marked(const offset_ptr<const gc_allocated> &p) {
      const std::size_t beg_word = p.offset() >> 3;
      return is_marked(compute_bitmap_index(beg_word), compute_bit_number(beg_word));
    }
    //For nullptr we return true. To be used by weak_gc_ptr barriers.
    bool is_marked(const offset_ptr<const gc_allocated> &p, bool check_null) {
      const std::size_t beg_word = p.offset() >> 3;
      return (!check_null || beg_word) ? is_marked(compute_bitmap_index(beg_word), compute_bit_number(beg_word)) : true;
    }

    bool is_marked(const base_offset_ptr *p) {
      const std::size_t beg_word = p->offset() >> 3;
      return is_marked(compute_bitmap_index(beg_word), compute_bit_number(beg_word));
    }

    void mark_weak(const offset_ptr<std::size_t> p) {
      return mark_weak(p.offset() >> 3);
    }

    bool mark_end_first(const offset_ptr<const gc_allocated> &p) {
      assert(p->get_gc_descriptor().is_valid());
      const std::size_t beg_word = p.offset() >> 3;
      const std::size_t end_word = beg_word + p->get_gc_descriptor().object_size() - 1;
      return _mark_end_first(beg_word, end_word);
    }

    void mark_begin_first(const offset_ptr<const gc_allocated> &p) {
      assert(p->get_gc_descriptor().is_valid());
      const std::size_t beg_word = p.offset() >> 3;
      const std::size_t end_word = beg_word + p->get_gc_descriptor().object_size() - 1;
      mark_begin_first(beg_word, end_word);
    }

    std::size_t find_next_free_word(std::size_t word, std::size_t end, bool &found_set_bit) const {
      bit_number_t bit = compute_bit_number(word);
      bitmap_idx_t idx = compute_bitmap_index(word);
      bitmap_idx_t end_idx = compute_bitmap_index(end);
      do {
        rep_t B = _end[idx] & construct_left_mask(bit);
        while (B == 0) {
          idx++;
          if (idx == end_idx) {
            return idx << value_log_bits;
          }
          B = _end[idx];
        }
        found_set_bit = true;
        if (B == 1) {
          idx++;
          if (idx == end_idx) {
            return idx << value_log_bits;
          }
          bit = 0;
        } else {
          bit = __builtin_clzl(B) + 1;
        }
      } while (_begin[idx] & construct_bitmap_word(bit));
      return (idx << value_log_bits) + bit;
    }

    std::size_t find_next_used_word(std::size_t word, std::size_t end) const {
      bit_number_t bit = compute_bit_number(word);
      bitmap_idx_t idx = compute_bitmap_index(word);
      bitmap_idx_t end_idx = compute_bitmap_index(end);
      if (idx >= end_idx) {
        return idx << value_log_bits;
      }
      rep_t B = _begin[idx] & construct_left_mask(bit);
      while (B == 0) {
        idx++;
        if (idx == end_idx) {
          return idx << value_log_bits;
        }
        B = _begin[idx];
      }
      return (idx << value_log_bits) + __builtin_clzl(B);
    }

    //Actually, this function returns the prev in-use word + 1.
    std::size_t find_prev_used_word(std::size_t word) const {
      bit_number_t bit = compute_bit_number(word);
      bitmap_idx_t idx = compute_bitmap_index(word);
      rep_t B = _end[idx];
      B &= construct_right_mask(bit);
      while (B == 0) {
        if (idx == 0) {
          return 0;
        }
        B = _end[--idx];
      }
      return (idx << value_log_bits) + (bits_per_value - __builtin_ctzl(B));
    }

    void reset_logical_chunk_count() {
      _logical_chunks = 0;
      _sweep_bitmap_words = 0;
    }

    //nr_chunk: chunk number from where to start.
    std::size_t process_next_chunk_begin(std::size_t nr_chunk, const bool set_bit) {
      constexpr std::size_t bits_to_shift = chunk_size_log_bits + value_log_bits;
      std::size_t start = nr_chunk << bits_to_shift;
      std::size_t end = (nr_chunk + 1) << bits_to_shift;
      while (start < (_size << value_log_bits)) {
        start = find_next_used_word(start, end);
        if (start < end) {
          /* TODO: We can have an optimization here. If the set bit is the
           * last bit of the _begin chunk, then we can set the _end bitmap
           * to be clear. Also, by just resetting the last bit of _begin
           * bitmap, even it can be set to be clear.
           */
          break;
        }
        assert(start == end);
        end += 1 << bits_to_shift;
        //atomically set bits. No need to clear _begin as we are here coz its all cleared
        set_sweep_bitmap_both(nr_chunk, set_bit);
        nr_chunk++;
      }
      return start;
    }

    void test_bitmaps(const bool set_bitmap) {
      volatile std::size_t i;
      for(i = 0; i < _sweep_bitmap_size; i++) {
        set_bitmap ? assert(_sweep_bitmap_begin[i] == rep_t(-1) && _sweep_bitmap_end[i] == rep_t(-1)) :
                     assert(_sweep_bitmap_begin[i] == 0 && _sweep_bitmap_end[i] == 0);
      }

      for(i = 0; i < _size; i++) {
        assert(_begin[i] == 0);
        assert(_end[i] == 0);
        assert(_weak[i] == 0);
      }
    }

    bool expand_free_chunk(std::size_t*,
                           offset_ptr<gc_allocator::global_chunk>,
                           std::size_t, std::size_t&, std::size_t&);
    void post_sweep_phase(per_process_struct*, const bool);
    bool post_sweep_phase_without_load_balancing(per_process_struct*, const bool);
    void post_sweep_clear(const std::size_t, const bool);
    void process_logical_chunk(gc_control_block&,
                               gc_allocator::skiplist&,
                               const std::size_t,
                               const bool);
    void _cleanup_sweep1_phase(per_process_struct*, gc_allocator::skiplist&, const bool);
    void cleanup_weak_ptrs(gc_control_block&, std::size_t, const std::size_t);
    void verify_weak_ptrs_cleanup();
    void verify_weak_ptr_cleanup(std::size_t*);
    //void cleanup_weak_ptr(std::size_t*);
    void atomic_cleanup_weak_ptr(gc_control_block&, std::size_t*);
    void set_sweep_bitmap_range(const std::size_t, const std::size_t, const bool);
    void expand_and_put_chunk(gc_control_block&, gc_allocator::skiplist&, chunk_expansion_slot&, const bool, std::mt19937&);
    void sweep1_phase(gc_control_block&, chunk_expansion_slot&, std::mt19937&,const uint8_t, const bool, const bool);
    void sweep2_phase(const bool);
    void mark_gc_control_block();
  };
}

#endif /* GC_GC_THREAD_H_ */

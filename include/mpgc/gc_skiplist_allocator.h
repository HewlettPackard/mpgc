/*
 *
 *  Multi Process Garbage Collector
 *  Copyright Â© 2016 Hewlett Packard Enterprise Development Company LP.
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Lesser General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the 
 *  Application containing code generated by the Library and added to the 
 *  Application during this compilation process under terms of your choice, 
 *  provided you also meet the terms and conditions of the Application license.
 *
 */

#ifndef GC_SKIPLIST_ALLOCATOR_H
#define GC_SKIPLIST_ALLOCATOR_H

#include <cstring>
#include <random>
#include <utility>
#include <map>
#include <stack>
#include <atomic>
#include "mpgc/gc_fwd.h"
#include "mpgc/offset_ptr.h"

#include "ruts/mash.h"
#include "ruts/atomic16B.h"
#include "ruts/managed.h"

namespace mpgc {
  struct gc_control_block;
  namespace gc_handshake {
    struct in_memory_thread_struct;
  }

  /* The main allocator namespace. */
  namespace gc_allocator {
    union slot_number {
      struct {
        uint16_t sentinel_idx;
        uint16_t block_idx;
      };
      uint32_t data;
      slot_number() : data(0) {}
      slot_number(uint16_t b) : sentinel_idx(0), block_idx(b) {}
      slot_number(uint16_t s, uint16_t b) : sentinel_idx(s), block_idx(b) {}
    };

    struct slot;

    constexpr static std::size_t bits_in_word() {
      return sizeof(void*) * 8;
    }
    
    /* We must chose an odd number for max_level so that the size of head skip_node
     * is odd. This is needed as the last two fields of tail skip_node, which immediately
     * follows the head skip_node, needs to be 16-byte aligned as its a 128-bit atomic field.
     */
    constexpr static std::size_t slab_size = 1 << 17;//slab_size is in words
    //Don't use a max_level of the form (2^i - 1) as that pattern is used to indicate last skip node.
    constexpr static uint8_t max_level = 10;
    constexpr static std::size_t nr_slots = 1 << 10;
    constexpr static uint8_t nr_key_bits = __builtin_clzl(max_level);
    constexpr static auto key_fld = bits::field<std::size_t, std::size_t>(0, nr_key_bits);
    constexpr static auto level_fld = bits::field<uint8_t, std::size_t>(nr_key_bits, bits_in_word() - nr_key_bits);

    constexpr static std::size_t alignment = sizeof(std::size_t);
    constexpr static std::size_t alignment_log = __builtin_ctzl(alignment);

    class local_chunk {
      std::size_t _size;
      local_chunk* _next;

     public:
      local_chunk() = delete;
      local_chunk(const local_chunk&) = default;
      local_chunk(std::size_t size, local_chunk* other) : _size(size), _next(other)
      {}

      local_chunk* &next() { return _next;}
      local_chunk* next() const { return _next;}
      std::size_t size() const { return _size; }
      void set_next(local_chunk* n) { _next = n; }
    };

    class global_chunk {
      /* _next, which is a offset_ptr, should *NOT* be the first word of global
       * chunk as that may be confused with an external gc_descriptor by async
       * signal handler.
       */
      std::size_t _size;
      offset_ptr<global_chunk> _next;

     public:
      global_chunk() = delete;
      global_chunk(const global_chunk&) = default;
      explicit global_chunk(std::size_t s) : _size(s), _next(nullptr)
      {}

      std::size_t size() const              {return _size;}
      offset_ptr<global_chunk>& next()      { return _next; }
      offset_ptr<global_chunk> next() const { return _next; }

      void set_size(std::size_t s) {
        _size = s;
      }
      void set_next(offset_ptr<global_chunk> c) {_next = c;}
    };
 }//gc_allocator

  struct chunk_expansion_slot {
    std::size_t size = 0;
    offset_ptr<gc_allocator::global_chunk> ptr = nullptr;
    chunk_expansion_slot() = default;
    chunk_expansion_slot(std::size_t s, offset_ptr<gc_allocator::global_chunk> p)
      : size(s), ptr(p) {}
    void clear() {
      size = 0;
      ptr = nullptr;
    }
  };

 namespace gc_allocator {
    constexpr static std::size_t min_global_chunk_size() {
      return sizeof(global_chunk) >> alignment_log;
    }

    constexpr static inline std::size_t align_size_up(std::size_t size, std::size_t alignment) {
      return (size + (alignment - 1)) & ~(alignment - 1);
    }

   /*
    * We need to be able to copy and assign a bump_chunk from a
    * volatile bump chunk.  This requires a constructor and assignment
    * operator that takes a volatile (or const volatile) ref, which
    * can't be defaulted and so needs a body.  GCC 5.4.0 insists that
    * this means that bump_chunk isn't trivially copyable (I believe
    * it's wrong) and we get a static assertion failure when we try to
    * make an atomic<bump_chunk> inside of atomic16B<bump_chunk>.
    * (This all worked in 4.9.2/3.)
    *
    * The fix I'm going with is to get rid of the assignment operator
    * and add a two-argument constructor that takes a volatile ref and
    * an indicator object.  This seems to make GCC 5.4.0 happy.
    */
    struct bump_chunk{
      uint64_t begin = 0;
      uint64_t end = 0;
      bump_chunk() = default;
      bump_chunk(uint64_t b, uint64_t e) : begin{b}, end{e} {}
      bump_chunk(const bump_chunk &) = default;
      bump_chunk& operator=(const bump_chunk &c) = default;
      struct from_volatile_t {};
      constexpr static from_volatile_t from_volatile{};

      bump_chunk(from_volatile_t, volatile bump_chunk &c)
        : begin{c.begin}, end{c.end}
      {}
    };
   
    struct skip_node {
      static std::atomic<std::size_t> n_skip_nodes;
      union {
        const std::size_t level_key = 0;//<level, key> pair
        std::atomic<std::size_t> level_orig_end;//For tail node
      };
      struct value_next {
        std::atomic<offset_ptr<global_chunk>> val;
        std::atomic<offset_ptr<skip_node>> next[1];
      };
      union {
        value_next val_next;
        ruts::atomic16B<bump_chunk> atomic_bump_ptr;
        volatile bump_chunk         bump_ptr;
      };

      skip_node(uint8_t level, std::size_t key, offset_ptr<skip_node> n = nullptr)
        : level_key(level_fld.encode(level) | key_fld.encode(key))
      {
        val_next.val = nullptr;
        val_next.next[0] = n;
        //n_skip_nodes++;
      }

      void clear_val() {
        val_next.val = nullptr;
        std::atomic_signal_fence(std::memory_order_release);
        val_next.next[0] = nullptr;
      }
      void set_next(offset_ptr<skip_node> n) {
        for (uint8_t i = 0; i <= max_level; i++) {
          val_next.next[i] = n;
        }
      }
    };

    class skiplist {
      skip_node head;
      /* Dirty hack, but doing to save space. We need head node to
       * be of max level. But the tail node can be at level 0. The
       * higher_levels array is to make space for that full array.
       */
      std::atomic<offset_ptr<skip_node>> higher_levels[max_level];
      skip_node tail;

      offset_ptr<skip_node> fast_lookup_cache[nr_slots];
      ruts::atomic16B<chunk_expansion_slot> chunk_expansion_slots[nr_slots];

      uint8_t choose_top_level(std::mt19937 &rand) const {
        uint8_t ret;
        do {
          ret = __builtin_clz(rand());
        } while (ret > max_level);
        return ret;
      }

      bool is_last_node(offset_ptr<skip_node> node) {
        return node == &tail;
      }

      offset_ptr<skip_node> search(const std::size_t size,
                                   offset_ptr<skip_node> *preds = nullptr,
                                   offset_ptr<skip_node> *succs = nullptr) {
        assert(size >= 2);
        if (size == 2) {
          return &head;
        } else if (size - 3 < nr_slots) {
          offset_ptr<skip_node> &node = fast_lookup_cache[size - 3];
          if (node != nullptr) {
            return node;
          } else if (preds == nullptr) {
            for (uint16_t i = size - 2; i < nr_slots; i++) {
              offset_ptr<skip_node> &n = fast_lookup_cache[i];
              if (n != nullptr) {
                return n;
              }
            }
          }
        }
        offset_ptr<skip_node> pred = &head, curr;
        for (uint8_t level = max_level + 1; level > 0; level--) {
          curr = pred->val_next.next[level - 1];
          while (!is_last_node(curr) && key_fld.decode(curr->level_key) < size) {
            pred = curr;
            curr = curr->val_next.next[level - 1];
          }
          if (preds) {
            assert(succs);
            preds[level - 1] = pred;
            succs[level - 1] = curr;
          }
        }
        return curr;
      }

      void insert_chunk(offset_ptr<skip_node> node, offset_ptr<global_chunk> chunk) {
        offset_ptr<global_chunk> expected = node->val_next.val;
        do {
          chunk->set_next(expected);
        } while (!node->val_next.val.compare_exchange_weak(expected, chunk));
      }

      constexpr static auto     slot_fld = bits::field<uint16_t, uint64_t>(49, 15);
      constexpr static auto bump_ptr_fld = bits::field<uint64_t, uint64_t>(0, 49);

      void help_inserting_bump_chunk(std::size_t expected_level_key, bump_chunk &exp) {
        if (exp.begin != 0 && exp.end == 0) {
          std::size_t beg_word = exp.begin;
          std::size_t end_word = beg_word - 1 + *reinterpret_cast<std::size_t*>(base_offset_ptr::base() +
                                                                                (beg_word << alignment_log));
          bump_chunk des(bump_chunk::from_volatile, tail.bump_ptr);
          if (des.begin == exp.begin && des.end == 0) {
            tail.level_orig_end.compare_exchange_strong(expected_level_key,
                                                        key_fld.replace(expected_level_key, end_word));
            des.begin = exp.begin;
            des.end = end_word;
            if (tail.atomic_bump_ptr.compare_exchange_strong(exp, des)) {
              exp = des;
            }
          } else {
            exp = des;
          }
        }
      }

      void move_bump_chunk_for_expansion(bump_chunk &exp, std::atomic<std::size_t>* atomic_size_ptr,
                                         std::size_t prev_size, const std::size_t size) {
        if (exp.begin != 0 && exp.begin == exp.end) {
          atomic_size_ptr->compare_exchange_strong(prev_size, size);
          offset_ptr<global_chunk> c = reinterpret_cast<global_chunk*>(atomic_size_ptr);
          //then put the bump ptr in expansion slot and then remove it.
          //This works on the assumption that there has to be >= slots available in expansion array than the
          //number of slots that we may want to add here.

          if (size >= 256) {
            for (uint16_t iter = 0; iter < nr_slots; iter++) {
              chunk_expansion_slot exp_slot(0, nullptr);
              chunk_expansion_slot des_slot(size, c);
              if (chunk_expansion_slots[iter].compare_exchange_strong(exp_slot, des_slot) ||
                  (exp_slot.size == size && exp_slot.ptr == c)) {
                break;
              }
            }
          }
          bump_chunk des;
          tail.atomic_bump_ptr.compare_exchange_strong(exp, des);
        }
      }

      bool replace_bump_chunk(gc_control_block &cb, offset_ptr<global_chunk> chunk) {
        //first pull out the chunk and put it in chunk_expansion_slots
        //first read the bump ptr
        bump_chunk exp{bump_chunk::from_volatile, tail.bump_ptr};
        //then read the key
        std::size_t end_word = key_fld.decode(tail.level_orig_end);
        std::atomic<std::size_t> *atomic_size_ptr = reinterpret_cast<std::atomic<std::size_t>*>(base_offset_ptr::base() +
                                                                (exp.begin << alignment_log));
        std::size_t prev_size = atomic_size_ptr->load();

        //then again read the bump ptr. If it is still the same, then perform store.
        bump_chunk exp1{bump_chunk::from_volatile, tail.bump_ptr};
        std::size_t size = end_word - exp.begin + 1;
        slot_number slot1(slot_fld.decode(exp.begin),
                          slot_fld.decode(exp.end));
        slot_number slot2;
        if (exp.begin == exp1.begin && exp.end == exp1.end &&
            bump_ptr_fld.decode(exp.begin) == bump_ptr_fld.decode(exp.end)) {
          //Deal with in-flight allocation from begin (slot field not empty).
          _help_unfinished_bump_alloc(cb, exp, exp1, slot1, slot2, true);

          move_bump_chunk_for_expansion(exp, atomic_size_ptr, prev_size, size);

          bool work_done = false;
          //pull out the largest chunk from the skiplist and insert it here.
          iterate_skipnodes([] {return true;}, [this, &work_done](offset_ptr<skip_node> &n) {
               offset_ptr<global_chunk> exp = n->val_next.val;
               if (key_fld.decode(n->level_key) < (sizeof(skip_node) >> alignment_log)) {
                 return false;
               }
               while (exp) {
                 //first take out the chunk
                 if (!n->val_next.val.compare_exchange_weak(exp, exp->next())) {
                   continue;
                 }
                 //then try inserting it in bump ptr
                 if (!insert_chunk_for_bump_alloc(exp)) {
                   //if failed, then put it back
                   offset_ptr<global_chunk> d = exp;
                   exp = n->val_next.val;
                   do {
                     d->set_next(exp);
                   } while(!n->val_next.val.compare_exchange_weak(exp, d));
                 }
                 work_done = true;
                 return false;
               }
               return true;
            });

            if (work_done) {
              return true;
            } else {
              //Couldn't put a new chunk as bump chunk. Try with 'chunk'
              return !insert_chunk_for_bump_alloc(chunk);
            }
        }
        return true;
      }

      offset_ptr<skip_node> allocate_skip_node(gc_control_block &cb,
                                               uint8_t &level,
                                               const std::size_t size,
                                               offset_ptr<global_chunk> c) {
        std::size_t alloc_sz = (level * sizeof(offset_ptr<skip_node>) + sizeof(skip_node)) >> alignment_log;
        bump_chunk exp{bump_chunk::from_volatile, tail.bump_ptr};
        std::atomic_signal_fence(std::memory_order_release);
        bump_chunk des;
        do {
          //Deal with the case where exp.end is 0 and exp.begin is not. Means someone is inserting a new chunk.
          if (exp.begin != 0 && exp.end == 0) {
            std::size_t exp_level_key = tail.level_orig_end;
            bump_chunk exp1{bump_chunk::from_volatile, tail.bump_ptr};
            if (exp.begin == exp1.begin) {
              //exp is passed by reference, so will be updated.
              help_inserting_bump_chunk(exp_level_key, exp);
            } else {
              exp = exp1;
              continue;
            }
          }
          std::size_t diff = bump_ptr_fld.decode(exp.end) - bump_ptr_fld.decode(exp.begin);
          if (diff >= alloc_sz) {
            des.begin = exp.begin;
            uint64_t target_end = bump_ptr_fld.decode(exp.end) - alloc_sz;
            des.end = bump_ptr_fld.replace(exp.end, target_end);
            if (tail.atomic_bump_ptr.compare_exchange_strong(exp, des)) {
              skip_node *ret = new (base_offset_ptr::base() + ((target_end + 1)<< alignment_log)) skip_node(level, size);
              return ret;
            }
          } else {
            if (diff > 0) {
              //pull out whatever is remaining.
              des.begin = des.end = exp.begin;
              if (tail.atomic_bump_ptr.compare_exchange_strong(exp, des)) {
                if (diff >= (sizeof(skip_node) >> alignment_log)) {
                  //If we have enough space to fit a skip node with level 0, we'll do it
                  level = diff - (sizeof(skip_node) >> alignment_log);
                  skip_node *ret = new (base_offset_ptr::base() + ((des.begin + 1) << alignment_log)) skip_node(level, size);
                  return ret;
                } 
              } else {
                continue;
              }
            }
            if (!replace_bump_chunk(cb, c)) {
              //This means that chunk c has been added as bump chunk and hence there is no need to allocate skip node.
              return nullptr;
            }
            exp = bump_chunk{bump_chunk::from_volatile, tail.bump_ptr};
          }
        } while(true);
      }
      bool _help_unfinished_bump_alloc(gc_control_block&, bump_chunk&, bump_chunk&,
                                       slot_number&, slot_number&, bool);

      offset_ptr<global_chunk> bump_pointer_allocate(gc_control_block&,
                                                     slot_number&,
                                                     const std::size_t);
     public:
      /* head node keeps list of 2-word chunks. Smaller than that
       * are dropped on the floor with word count written in first
       * word.
       */
      skiplist() : head(max_level, 2, &tail),
                   tail(level_fld.max_val(), 0, nullptr) {
        for (int i = 0; i < max_level; i++) {
          higher_levels[i] = &tail;
        }
      }

      ruts::atomic16B<chunk_expansion_slot>& expansion_slot_ref(uint16_t i) {
        return chunk_expansion_slots[i];
      }

      void set_tail(offset_ptr<global_chunk> p, std::size_t size) {
        bump_chunk chunk;
        chunk.begin = p.offset() >> 3;
        chunk.end = chunk.begin + size - 1;
        tail.level_orig_end = level_fld.encode(level_fld.max_val()) | key_fld.encode(chunk.end);
        tail.atomic_bump_ptr = chunk;
      }

      void reset() {
       head.clear_val();
       head.set_next(&tail);
       tail.clear_val();
       std::memset(fast_lookup_cache, 0x0, sizeof(fast_lookup_cache));
       std::memset(chunk_expansion_slots, 0x0, sizeof(chunk_expansion_slots));
       /*std::cout << std::endl
                 << "# OF SKIP NODES LAST TIME: "
                 << skip_node::n_skip_nodes
                 << std::endl
                 << std::endl;
       skip_node::n_skip_nodes = 0;*/
       
      }

      skip_node& tail_node() { return tail;}

      void help_unfinished_bump_alloc();

      bool insert_chunk_for_bump_alloc(offset_ptr<global_chunk> chunk) {
        std::size_t beg_word = chunk.offset() >> alignment_log;
        bump_chunk exp;
        bump_chunk des;
        des.begin = beg_word;
        //Need to read bump_ptr to ensure the key read is legit.
        exp = bump_chunk{bump_chunk::from_volatile, tail.bump_ptr};
        if (exp.end != 0) {
          //Somebody has successfully installed a new chunk.
          return false;
        }
        //read current key;
        std::size_t expected_level_key = tail.level_orig_end;
        bump_chunk exp1{bump_chunk::from_volatile, tail.bump_ptr};
        if (exp.begin != 0 && exp1.begin != exp.begin) {
          return false;
        }
        std::size_t end_word = beg_word + chunk->size() - 1;
        if (exp1.begin == 0 && tail.atomic_bump_ptr.compare_exchange_strong(exp, des)) {
          //cas key to end word, and then cas to set end in bump ptr
          tail.level_orig_end.compare_exchange_strong(expected_level_key,
                                                      key_fld.replace(expected_level_key, end_word));
          exp = des;
          des.end = end_word;
          tail.atomic_bump_ptr.compare_exchange_strong(exp, des);
          return true;
        } else {
          help_inserting_bump_chunk(expected_level_key, exp);
          return false;
        }
      }

      void insert(gc_control_block &cb,
                  offset_ptr<global_chunk> chunk,
                  std::mt19937 &rand) {
        const std::size_t size = chunk->size();
        offset_ptr<skip_node> preds[max_level + 1];
        offset_ptr<skip_node> succs[max_level + 1];
        offset_ptr<skip_node> node = nullptr;
        uint8_t toplevel = 0;
        while(true) {
          offset_ptr<skip_node> curr = search(size, preds, succs);
          if (key_fld.decode(curr->level_key) == size) {
            insert_chunk(curr, chunk);
          } else {
            if (!node) {
              toplevel = choose_top_level(rand);
              node = allocate_skip_node(cb, toplevel, size, chunk);
              if (node == nullptr) {
                //This means the chunk that we wanted to insert has been inserted as bump chunk.
                return;
              }
              node->val_next.val = chunk;
            }
            for (uint8_t level = 0; level <= toplevel; level++) {
              node->val_next.next[level] = succs[level];
            }
            offset_ptr<skip_node> succ = succs[0];
            offset_ptr<skip_node> pred = preds[0];
            if (!pred->val_next.next[0].compare_exchange_strong(succ, node)) {
              continue;
            }
            //Add node in the dast lookup cache.
            if (size - 3 < nr_slots) {
              fast_lookup_cache[size - 3] = node;
            }
            for (uint8_t level = 1; level <= toplevel; level++) {
              pred = preds[level];
              succ = succs[level];
              while(!pred->val_next.next[level].compare_exchange_strong(succ, node)) {
                while (!is_last_node(succ) && key_fld.decode(succ->level_key) < size) {
                  pred = succ;
                  succ = succ->val_next.next[level];
                }
                node->val_next.next[level] = succ;
              }//while
            }//for
          }//if
          break;
        }
      }

      offset_ptr<global_chunk> allocate(gc_control_block &cb,
                                        slot_number &sn,
                                        const std::size_t req_size,
                                        const std::size_t max,
                                        std::mt19937 &rand) {
        //First search for the size-sized chunk in the skiplist
        offset_ptr<skip_node> node = search(req_size);
        offset_ptr<global_chunk> chunk;
        while(true) {
          if (is_last_node(node)) {
            chunk = bump_pointer_allocate(cb, sn, req_size);
            break;
          } else {
            chunk = node->val_next.val;
            while (chunk && !node->val_next.val.compare_exchange_weak(chunk, chunk->next())) {
              //Do Nothing.
            }
            if (!chunk) {
              node = node->val_next.next[0];
            } else {
              const std::size_t chunk_size = chunk->size();
              if (chunk_size > max && (chunk_size - max) >= min_global_chunk_size()) {
                //Chunk is too big. We need to chop out what we need and put rest back to list.
                node = search(chunk_size - max);
                std::size_t put_back_size = key_fld.decode(node->level_key);
                std::size_t * const c = reinterpret_cast<std::size_t*>(chunk.as_bare_pointer());
                //We know that node can't be bigger than earlier node. So we don't need to check for it being last node.
                if (put_back_size > chunk_size - req_size) {
                  put_back_size = chunk_size - max;
                  offset_ptr<global_chunk> put_back = new (c + max) global_chunk(put_back_size);
                  /* The fence is required to ensure that the put_back_size is written first and then the
                   * returning chunks size is changed. Otherwise, if it gets reordered, and a thread carshes
                   * before setting the put_back chunk's size, then we will some GC threads may read garbage
                   * in erase_gc_descriptors() function. The same applies to the fence in else below.
                   */ 
                  std::atomic_signal_fence(std::memory_order_release);
                  chunk->set_size(max);
                  insert(cb, put_back, rand);
                } else {
                  //Found a skip node in [min_chop, max_chop] range.
                  const std::size_t ret_size = chunk_size - put_back_size;
                  offset_ptr<global_chunk> put_back = new (c + ret_size) global_chunk(put_back_size);
                  std::atomic_signal_fence(std::memory_order_release);
                  chunk->set_size(ret_size);
                  insert_chunk(node, put_back);
                }
              }
              break;
            }
          }
        }//while
        return chunk;
      }

      static offset_ptr<global_chunk> get_from_global(gc_handshake::in_memory_thread_struct &, const std::size_t);

      template <typename Continue, typename Func>
      void iterate_skipnodes(Continue &&cont, Func &&func) {
        using nodep = offset_ptr<skip_node>;
        constexpr std::size_t level = 4;
        std::stack<nodep> level_n, level_0;
        nodep end = &tail;
        uint8_t test = 0;
        for (nodep i = &head; i != end; i = i->val_next.next[level]) {
          //If we are reading garbage, means other GC threads have started expansion.
          if (!i.is_valid()) {
            return;
          } else if (test++ == 5) {
            if (!std::forward<Continue>(cont)()) {
              return;
            }
            test = 0;
          }
          level_n.push(i);
        }
        while (!level_n.empty()) {
          for (nodep i = level_n.top(); i != end; i = i->val_next.next[0]) {
            if (!i.is_valid()) {
              return;
            } else if (test++ == 5) {
              if (!std::forward<Continue>(cont)()) {
                return;
              }
              test = 0;
            }
            level_0.push(i);
          }
          end = level_n.top();
          level_n.pop();
          while (!level_0.empty()) {
            //func returns true if it wants us to continue.
            if (!std::forward<Func>(func)(level_0.top())) {
              return;
            }
            level_0.pop();
          }
        }
      }
    };

    using localPoolType = std::map<std::size_t, local_chunk*>;
   extern void* alloc (gc_handshake::in_memory_thread_struct&, std::size_t, std::size_t);
  }//gc_allocator
}//mpgc

namespace ruts {
  template<> struct hash1<mpgc::gc_allocator::global_chunk*> {
    std::uint64_t operator()(mpgc::gc_allocator::global_chunk* p) const {
      static masher m1(5);
      return m1(reinterpret_cast<uint64_t>(p));
    }
  };

  template<> struct hash2<mpgc::gc_allocator::global_chunk*> {
    std::uint64_t operator()(mpgc::gc_allocator::global_chunk* p) const {
      static masher m1(17);
      return m1(reinterpret_cast<uint64_t>(p));
    }
  };
}

#endif //GC_SKIPLIST_ALLOCATOR_H

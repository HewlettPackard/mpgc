/*
 *
 *  Multi Process Garbage Collector
 *  Copyright Â© 2016 Hewlett Packard Enterprise Development Company LP.
 *
 *  This program is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU Lesser General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the 
 *  Application containing code generated by the Library and added to the 
 *  Application during this compilation process under terms of your choice, 
 *  provided you also meet the terms and conditions of the Application license.
 *
 */

/*
 * gc.h
 *
 *  Created on: Apr 15, 2015
 *      Author: Evan
 */

#ifndef GC_H_
#define GC_H_

#include "mpgc/gc_fwd.h"
#include "mpgc/offset_ptr.h"
#include "mpgc/gc_desc.h"
#include "mpgc/gc_allocated.h"
#include "mpgc/gc_array.h"
#include "mpgc/gc_skiplist_allocator.h"
#include "mpgc/gc_ptr.h"
#include "mpgc/gc_allocate.h"
#include "mpgc/gc_versioned.h"
#include "mpgc/gc_virtuals.h"
#include "mpgc/gc_thread.h"
#include "mpgc/gc_handshake.h"
#include "mpgc/external_gc_ptr.h"
#include "mpgc/gc_cuckoo_map.h"
#include "mpgc/weak_gc_ptr.h"
#include "mpgc/contingent_gc_ptr.h"
#include "mpgc/weak_ctrl_map.h"
#include "mpgc/bump_allocation_slots.h"

#include "ruts/collections.h"
#include "ruts/managed.h"

namespace mpgc {
  typedef ruts::parallel_lazy_delete_collection<per_process_struct, ruts::managed_space::allocator<per_process_struct>> perProcessList;

  enum class Stage : unsigned char {
    Sweeped,
    preTracing,
    Tracing,
    Traced,
    preSweeping,
    Sweeping
  };

  enum struct Weak_stage {
    Normal = 0,
    Trace = 1,
    Repeat = 2,
    Clean = 3
  };

  struct versioned_pcount_t {
    pcount_t count;
    pcount_t version;
    versioned_pcount_t() : count(0), version(0) {}
    versioned_pcount_t(pcount_t c) : count(c), version(0) {}
    versioned_pcount_t(pcount_t c, pcount_t v) : count(c), version(v) {}
  };

  class gc_mem_stats {
    std::atomic<std::size_t> gc_cycle_num;
    offset_ptr<gc_control_block> cblk;
    const std::size_t heap_size;
    std::atomic<std::size_t> in_use_stable;
    std::atomic<std::size_t> in_use_current;
    std::atomic<std::size_t> n_objects_stable;
    std::atomic<std::size_t> n_objects_current;
    friend class gc_control_block;
    gc_mem_stats(std::size_t hs, offset_ptr<gc_control_block> cb)
      : gc_cycle_num(0), cblk(cb), heap_size(hs),
	in_use_stable(0), in_use_current(0),
	n_objects_stable(0), n_objects_current(0)
    {}
  public:
    std::size_t bytes_in_heap() const {
      return heap_size;
    }
    std::size_t bytes_in_use() const {
      return in_use_stable;
    }
    std::size_t bytes_currently_in_use() const {
      return in_use_current;
    }
    std::size_t bytes_free() const {
      return bytes_in_heap()-bytes_in_use();
    }
    std::size_t cycle_number() const {
      return gc_cycle_num;
    }

    std::size_t n_processes() const;

    std::size_t n_objects() const {
      return n_objects_stable;
    }
    std::size_t n_current_objects() const {
      return n_objects_current;
    }
    std::size_t inc_cycle_num_to(std::size_t n) {
      std::size_t expected = n-1;
      if (gc_cycle_num.compare_exchange_strong(expected, n)) {
	// Yes, there's a window when these are out of sync, and yes,
	// it's possible for the process to die in between.  I'm not
	// terribly worried.
	in_use_stable = in_use_current.load();
	in_use_current = 0;
	n_objects_stable = n_objects_current.load();
	n_objects_current = 0;
	return n;
      } else {
	return expected;
      }
    }
    void marked(const offset_ptr<const gc_allocated> &p) {
      std::size_t bytes = p->get_gc_descriptor().object_size()*8;
      in_use_current += bytes;
      n_objects_current++;
    }
  };

  struct persistent_root_key {
    ruts::uniform_key id;
    
    persistent_root_key() = delete;
    persistent_root_key(const persistent_root_key &) = default;
    persistent_root_key(persistent_root_key &&) = default;
    persistent_root_key &operator =(const persistent_root_key &) = default;
    persistent_root_key &operator =(persistent_root_key &&) = default;

    constexpr static auto computed = ruts::uniform_key::computed;
    
    persistent_root_key(const ruts::uniform_key &key) : id(key) {}
    persistent_root_key(const ruts::with_uniform_id &wuid) : id(wuid.id) {}

    template <typename T, typename = std::enable_if_t<std::is_base_of<ruts::with_uniform_id, T>::value>>
      persistent_root_key(const gc_ptr<T> &ptr) : id(ptr->id) {}

    template <typename T, typename = std::enable_if_t<std::is_base_of<ruts::with_uniform_id, T>::value>>
      persistent_root_key(const T *ptr) : id(ptr->id) {}

    template <typename C, typename T, typename A>
    persistent_root_key(const std::basic_string<C,T,A> &s) : id(computed, s) {}

    persistent_root_key(const char *s) : id(computed, s) {}

    template <typename T, std::enable_if_t<std::is_arithmetic<T>::value>>
    persistent_root_key(T n) : id(computed, n) {}

    bool operator==(const persistent_root_key &other) const {
      return id == other.id;
    }
    bool operator!=(const persistent_root_key &other) const {
      return id != other.id;
    }
    

    template <typename CharT, typename Traits>
    friend std::basic_ostream<CharT, Traits> &operator <<(std::basic_ostream<CharT, Traits> &out,
                                                          const persistent_root_key &key)
    {
      return out << key.id;
    }

    static const auto &descriptor() {
      using this_type = persistent_root_key;
      static gc_descriptor d =
	GC_DESC(this_type)
	.template WITH_FIELD(&this_type::id);
      return d;
    }
  };

  struct persistent_roots_t {
    /*
     * Persistent roots are those that need to stick around even after
     * all processes that care about it have gone (because future
     * processes might).  The main examples are control blocks for
     * various subsystems. The roots are established and looked up by
     * key, and we assume that key allocation takes place by some
     * external process.  We also provide a way to say "Atomically
     * establish a value if one doesn't exist.  For now, we simply use
     * a smallish array.  This should probably be generalized.
     */

    using key_type = persistent_root_key;
    using ptr_type = gc_ptr<gc_allocated>;
    using map_type = small_gc_cuckoo_map<key_type, ptr_type>;

  private:
    mutable std::atomic<gc_ptr<map_type>> _map;
    gc_ptr<map_type> map() const {
      return _map;
    }
  public:

    persistent_roots_t &ensure_initialized() {
      if (map() == nullptr) {
        /*
         * Nobody's done this yet.
         */
        ruts::try_change_value(_map, nullptr, make_gc<map_type>(100));
        /*
         * If that didn't work, it means that somebody else got there
         * first and the map we created was dropped on the floor.
         * That's okay.
         */
      }
      return *this;
    }

    template <typename Fn>
    void enumerate_pointers(const Fn &fn) const {
      fn(map());
    }

    bool remove(key_type key) {
      return map()->remove(key);
    }

    bool contains(key_type key) {
      return map()->contains(key);
    }

    /*
     * Note that lookup doesn't check that it is actually of that
     * type.  It's assumed that the caller knows.
     */
    template <typename T>
    gc_ptr<T> lookup(key_type key) const {
      return std::static_pointer_cast<T>(map()->get(key));
    }

    /*
     * Note that swap doesn't check that the old value is actually
     * of that type.  It's assumed that the caller knows.
     */
    template <typename T>
    gc_ptr<T> swap(key_type key, const gc_ptr<T> &new_val) {
      auto rr = map()->put(key, new_val);
      return std::static_pointer_cast<T>(rr.old_value);
    }

    template <typename T>
    void store(key_type key, const gc_ptr<T> &new_val) {
      map()->put(key, new_val);
    }

    /*
     * Returns the resulting value (the one that was there or the one
     * we set).  Note that "new" means "There was no value there".  A
     * value of null counts as a value.
     */
    template <typename T>
    gc_ptr<T> store_new(key_type key, const gc_ptr<T> &new_val) {
      gc_ptr<map_type> m = map();

      bool has_val;
      ptr_type current;
      std::tie(has_val, current) = m->lookup(key);

      if (has_val) {
        return std::static_pointer_cast<T>(current);
      }

      auto rr = m->put_new(key, new_val);
      if (rr.replaced) {
        return new_val;
      } else {
        return std::static_pointer_cast<T>(rr.old_value);
      }
    }

    template <typename T>
    bool replace(key_type key, gc_ptr<T> &expected, const gc_ptr<T> &new_val) {
      auto rr = map()->replace(key, expected, new_val);
      if (!rr.replaced) {
        expected = std::static_pointer_cast<T>(rr.old_value);
      }
      return rr.replaced;
    }

    template <typename T, typename Fn, typename ... Args>
    gc_ptr<T> find_or(key_type key, const Fn &fn, Args && ...args)
    {
      gc_ptr<T> current = lookup<T>(key);
      if (current == nullptr) {
        gc_ptr<T> new_val = fn(std::forward<Args>(args)...);
        current = store_new(key, new_val);
      }
      return current;
    }
    template <typename T, typename ... Args>
    gc_ptr<T> find_or_create(key_type key, Args && ...args)
    {
      return find_or<T>(key, make_gc<T, Args...>, std::forward<Args>(args)...);
    }
  };

  struct gc_control_block {
    //We need 1 bit in expansion_slots_total below. So if the size of array
    //below needs to be larger than (1<<15), then use a type for the counter accordingly.
    std::array<gc_allocator::skiplist, 2> global_free_lists;
    gc_allocator::bump_allocation_slots bump_alloc_slots;

    persistent_roots_t persistent_roots;

    //declare bitmap class
    mark_bitmap bitmap;
    //declare gc_struct class here which contains per process
    perProcessList process_struct_list;

    gc_mem_stats mem_stats;

    weak_ctrl_map ctrl_map;

    //Total number of processes at any time
    std::atomic<versioned_pcount_t> total_process_count;

    //Barrier syncs for synchronizing between GC threads of all the active processes
    std::atomic<marking_barrier_type> marking_barrier;

    std::array<std::atomic<pcount_t>, std::size_t(Barrier_indices::arraysize)> barrier_sync;

    std::atomic<uint16_t> expansion_slots_counter;

    std::atomic<uint16_t> weak_stage;
    std::atomic<gc_status> status;

    std::atomic<Stage> stage;

    gc_control_block(std::size_t size, uint8_t* after_cblock) :
      bump_alloc_slots(after_cblock),
      bitmap(size),
      mem_stats(size, this),
      ctrl_map(nullptr),
      total_process_count(versioned_pcount_t()),
      marking_barrier(marking_barrier_type(Barrier_stage::incrementing, Barrier_indices::marking1)),
      weak_stage(0),
      status(gc_status(gc_handshake::Signum::sigSweep)),
      stage(Stage::Sweeped)
    {
      std::size_t size_bump_slots_block = bump_alloc_slots.sentinel[0]->get_gc_descriptor().object_size() << 3;
      std::size_t* first_free_word = reinterpret_cast<std::size_t*>(after_cblock + size_bump_slots_block);

      global_free_lists[0].set_tail(reinterpret_cast<gc_allocator::global_chunk*>(first_free_word),
                                    (size - sizeof(gc_control_block) - size_bump_slots_block) >> 3);

      for (uint8_t i = 0; i < barrier_sync.size(); i++) {
        barrier_sync[i] = 0;
      }
    }
  };

  extern gc_control_block &control_block();
  extern void init_on_createheap();

  template<typename T>
  inline
  typename white_allocator<T>::pointer white_allocator<T>::allocate(const size_t n) {
    static_assert(alignof(T) <= 8, "White allocation doesn't respect special alignment requests.");
    gc_control_block &cb = control_block();
    gc_status status = gc_handshake::process_struct->get_gc_status();
    return static_cast<T*>(cb.global_free_lists[status.status_idx.idx]
                               .allocate(cb, sizeof(T) * n));
  }

  inline
  persistent_roots_t &persistent_roots() {
    initialize_thread();
    static persistent_roots_t &pr = control_block().persistent_roots.ensure_initialized();
    return pr;
  }

  inline
  gc_mem_stats &memory_stats() {
    initialize_thread();
    return control_block().mem_stats;
  }

  template <typename Fn>
  auto gc_safe(Fn &&fn) {
    typename std::decay<Fn>::type f = std::forward<Fn>(fn);
    return [f](auto ...args) {
      initialize_thread();
      return f(args...);
    };
  }
}

namespace ruts {
  template <>
  struct hash1<mpgc::persistent_root_key> {
    auto operator()(const mpgc::persistent_root_key &key) const {
      return hash1<uniform_key>{}(key.id);
    }
  };
  template <>
  struct hash2<mpgc::persistent_root_key> {
    auto operator()(const mpgc::persistent_root_key &key) const {
      return hash2<uniform_key>{}(key.id);
    }
  };
}

namespace std {
  inline void cpu_relax() {
#ifdef __x86_64
    asm volatile("pause\n": : :"memory");
#elif defined(__aarch64__)
    asm volatile("yield\n": : :"memory");
#endif
  }
}

#include "mpgc/weak_barriers.h"

#endif /* GC_H_ */
